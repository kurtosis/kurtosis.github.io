<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-01-31T16:38:17-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kurt Smith</title><subtitle>Writeups of my side projects.</subtitle><entry><title type="html">Scaling Laws for Pass@k and Temperature</title><link href="http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling.html" rel="alternate" type="text/html" title="Scaling Laws for Pass@k and Temperature" /><published>2025-06-01T00:00:00-07:00</published><updated>2025-06-01T00:00:00-07:00</updated><id>http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling</id><content type="html" xml:base="http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#motivation" id="markdown-toc-motivation">Motivation</a></li>
  <li><a href="#toy-model" id="markdown-toc-toy-model">Toy Model</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a>    <ul>
      <li><a href="#temperature-dependence-for-a-single-task" id="markdown-toc-temperature-dependence-for-a-single-task">Temperature dependence for a single task</a></li>
      <li><a href="#how-does-t-scale-with-k-for-an-eval-set" id="markdown-toc-how-does-t-scale-with-k-for-an-eval-set">How does \(T^*\) scale with \(k\) for an eval set?</a>        <ul>
          <li><a href="#gaussian-task-difficulty" id="markdown-toc-gaussian-task-difficulty">Gaussian task difficulty</a></li>
          <li><a href="#exponential-task-difficulty" id="markdown-toc-exponential-task-difficulty">Exponential task difficulty</a></li>
          <li><a href="#power-law-task-difficulty" id="markdown-toc-power-law-task-difficulty">Power-law task difficulty</a></li>
          <li><a href="#comparing-all-three-task-distributions" id="markdown-toc-comparing-all-three-task-distributions">Comparing all three task distributions</a></li>
        </ul>
      </li>
      <li><a href="#deciding-how-large-to-make-k" id="markdown-toc-deciding-how-large-to-make-k">Deciding how large to make k</a></li>
    </ul>
  </li>
  <li><a href="#implications" id="markdown-toc-implications">Implications</a></li>
</ul>

<h1 id="summary">Summary</h1>

<p>\(pass@k\) is widely used for open-ended, verifiable benchmarks (e.g. code, formal proofs), but its nonlinear dependence on both \(k\) and temperature \(T\) makes it difficult to reason about. I use a toy model to study this dependence and find:</p>

<ul>
  <li>The optimal temperature \(T^*\) (that which maximizes \(\mathbb{E}[\text{pass@k}]\)) depends on \(k\). \(T^*(k)\) follows a power law over several decades of \(k\).
    <ul>
      <li>This behavior arises because eval sets contain tasks of varying difficulties (a “difficult task” being one with a low \(pass@1\)). For a single task, \(T^*\) is independent of \(k\).</li>
      <li>As \(k \rightarrow \infty\), \(T^*\) has an asymptotic upper bound that depends only on the single most difficult task in the eval set. For an infinite eval set with no hardest task, the power law scaling would continue indefinitely.</li>
    </ul>
  </li>
  <li>
    <p><strong>The scaling exponent of \(T^*(k)\) is governed by the tail of the task difficulty distribution - not its mean or variance.</strong> Gaussian, exponential, and power-law difficulty distributions yield qualitatively different \(T^*(k)\) behavior, even when matched on the first two moments. Concretely, the \(k\)-exponent increases from 0.73 (Gaussian) to 1.05 (exponential), meaning temperature should scale roughly linearly with \(k\) for exponential difficulty distributions but sub-linearly for Gaussian ones. The plots below show \(T^*(k)\) curves for increasingly fat-tailed eval distributions (Gaussian &lt; exponential &lt; power law).</p>

    <table>
  <tr>
    <td><img src="/assets/images_2025-06-01/image.png" width="300" /></td>
    <td><img src="/assets/images_2025-06-01/image_1.png" width="300" /></td>
  </tr>
  <tr>
    <td><img src="/assets/images_2025-06-01/image_2.png" width="300" /></td>
    <td><img src="/assets/images_2025-06-01/image_3.png" width="300" /></td>
  </tr>
</table>
  </li>
</ul>

<h1 id="motivation">Motivation</h1>

<p>Unlike greedy accuracy (binary per task, insensitive at low solve rates) or perplexity (dependent on a specific reference solution), \(pass@k\) is continuous-valued, increases with \(k\), and can be estimated to arbitrary precision.</p>

<p>However, \(pass@k\) introduces challenges that simpler metrics avoid:</p>

<ol>
  <li><strong><em>Sensitivity to temperature</em></strong>: using \(T=0\) for \(k&gt;1\) is suboptimal since all generations will be identical. It is not clear which \(T\) optimizes \(pass@k\), or whether comparing two models at the same \(T\) is apples-to-apples. (Model A may beat model B at \(T=0.5\) while model B wins at \(T=1\).)</li>
  <li><strong><em>Sensitivity to k</em></strong>: \(pass@k\) is a curve over \(1 \leq k &lt; \infty\). It is not obvious whether a particular set of \(k\) values provides the full picture when comparing two models.</li>
  <li><strong><em>Nonlinear interaction</em></strong>: \(pass@k\) has a nonlinear dependence on \(k\) and \(T\) that cannot easily be factored into independent effects, making it difficult to reason about analytically.</li>
</ol>

<p>I developed a toy model to isolate how \(pass@k\) depends on \(k\), \(T\), and the distribution of task difficulties across an eval set. The key questions:</p>

<ol>
  <li>For a single task, \(pass@k = 1 - (1-pass@1)^k\). But we report the average over a benchmark - how does the \(pass@k\) curve behave for a set of tasks with varying difficulty?</li>
  <li>For a given \(k\), there is an optimal \(T\) that maximizes \(pass@k\) (an exploration vs. exploitation trade-off). How does \(T^*(k)\) scale?</li>
  <li>Does the shape of the task difficulty distribution matter, beyond its mean and variance?</li>
</ol>

<h1 id="toy-model">Toy Model</h1>

<p>For any task prompt, assume there is some non-zero probability (at \(T&gt;0\)) that a given LLM will generate a correct solution. The probability of generating a specific token \(x_i\) scales with \(T\) as:</p>

\[p(x_i) \sim \exp(z_i / T)\]

<p>(where \(T\) also influences the normalization constant). Note the similarity to a zero-mean normal distribution:</p>

\[p(x) \sim \exp(-x^2 / 2 \sigma^2)\]

<p>Consider a discrete normal distribution (i.e. defined only on the integers \(\mathbb{Z}\)). This is equivalent to a softmax function over \(\mathbb{Z}\) where \(z_i = -x_i^2 / 2\) and \(T = \sigma^2\).</p>

<p>Since the possible generations of an LLM form a countable set, we can view this distribution as a simplified representation of an LLM (where each integer represents one generation). A task is represented by an integer \(c\): we sample a random integer \(g\) from the model distribution and say it passes if \(g=c\). Tasks with large \(\lvert c \rvert\) are “difficult” (low probability under the model) and tasks with small \(\lvert c \rvert\) are “easy.”</p>

<p>The model intentionally strips away semantic content to isolate the dependence of \(pass@k\) on temperature, sample count, and the distribution of task difficulties. The absence of task-specific prompts (i.e. we always generate from the same distribution) is not a limitation - what matters is that each task has a \(pass@1\) rate of \(p_{gen}(c \mid T)\) that depends on its difficulty \(\lvert c \rvert\) and the temperature \(T\).</p>

<p>The final element is the evaluation set \(\{c_i\}\). The task difficulties are sampled from a distribution \(p_{task}(c)\) (not to be confused with the generation distribution \(p_{gen}\)). I consider three families:</p>

<table>
  <thead>
    <tr>
      <th>Task distribution</th>
      <th>Form</th>
      <th>Key parameter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Gaussian</td>
      <td>\(p_{task}(c=j) \sim \exp(-j^2 / 2\sigma_{task}^2)\)</td>
      <td>\(\sigma_{task}\) (task_spread)</td>
    </tr>
    <tr>
      <td>Exponential</td>
      <td>\(p_{task}(c=j) \sim \exp(-\lvert j \rvert / \sqrt{T_{task}})\)</td>
      <td>\(T_{task}\)</td>
    </tr>
    <tr>
      <td>Power law</td>
      <td>\(p_{task}(c=j) \sim \lvert j \rvert^{-\gamma}\)</td>
      <td>\(\gamma\)</td>
    </tr>
  </tbody>
</table>

<p>These families have increasingly fat tails, allowing us to test whether the tail shape - independent of the first two moments - affects the scaling of \(T^*(k)\).</p>

<h1 id="results">Results</h1>

<h2 id="temperature-dependence-for-a-single-task">Temperature dependence for a single task</h2>

<p>Before examining eval sets, it is useful to understand how \(pass@1\) depends on \(T\) for a single task. For \(p_{gen}(c \mid T)\):</p>

<ol>
  <li>At \(T=0\), \(p_{gen}(c)=0\) unless \(c\) is the maximum-likelihood generation.</li>
  <li>As \(T\) increases, probability mass spreads from the mode toward \(c\), increasing \(p_{gen}(c \mid T)\).</li>
  <li>Beyond some optimal \(T\), more mass flows away from \(c\) toward the tails than flows in from the mode, and \(p_{gen}(c \mid T)\) decreases.</li>
  <li>Concretely, for a zero-mean normal distribution, \(p(x)\) is maximized when \(\sigma = \lvert x \rvert\). The plot below illustrates this - compare the values of each curve at \(x=1\):</li>
</ol>

<figure>
<img src="/assets/images_2025-06-01/image_17.png" width="500" />
<figcaption style="font-size: 0.85em; color: #666;">Source: <a href="https://en.wikipedia.org/wiki/Normal_distribution">Wikipedia</a></figcaption>
</figure>

<p>For a single task, this optimal \(T\) is independent of \(k\) (since \(pass@k = 1-(1-pass@1)^k\) is monotonically increasing in \(pass@1\)).</p>

<h2 id="how-does-t-scale-with-k-for-an-eval-set">How does \(T^*\) scale with \(k\) for an eval set?</h2>

<p>For a set of tasks the answer is different. I created eval sets by sampling tasks from each distribution family and calculated \(T^*(k)\) - the temperature maximizing \(pass@k\) for each \(k\) - using Brent’s method.</p>

<p>Key observations:</p>
<ul>
  <li>As \(k \rightarrow \infty\), \(pass@k\) is dominated by the most difficult task (the largest \(\lvert c \rvert\)). Asymptotically, \(T^*\) converges to the value that maximizes \(pass@1\) for that task. This asymptotic value is noisy since it depends on a single extreme task.</li>
  <li>For intermediate values of \(k\), \(T^*\) follows a smooth power law over several decades - this is the scaling regime I focus on.</li>
</ul>

<h3 id="gaussian-task-difficulty">Gaussian task difficulty</h3>

<p>The left plot shows the full \(T^*\) vs \(k\) curve (\(task\_spread\) is the standard deviation of the task distribution). The right plot shows only points with \(0.03 &lt; pass@k &lt; 0.98\), isolating the scaling regime. The bottom plot shows that \(T^*\) also follows a scaling law with respect to \(task\_spread\).</p>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <div>
    <img src="/assets/images_2025-06-01/image_4.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_5.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_6.png" />
  </div>
  <div></div>
</div>

<p>Fitting a regression in this scaling regime (<em>s = task_spread</em>):</p>

\[T^* \approx 0.264 \cdot k^{0.728} \cdot s^{1.283} \text{ ; } R^2=0.9996\]

<h3 id="exponential-task-difficulty">Exponential task difficulty</h3>

<p>The same analysis with tasks drawn from an exponential distribution yields different scaling exponents:</p>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <div>
    <img src="/assets/images_2025-06-01/image_7.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_8.png" />
  </div>
</div>

\[T^* \approx 0.117 \cdot k^{1.054} \cdot s^{0.957} \text{ ; } R^2= 0.9989\]

<p>The \(k\)-exponent increases from 0.73 to 1.05 - temperature must scale approximately linearly with \(k\) for exponentially-distributed task difficulties, compared to sub-linearly for Gaussian. This points to the central finding: <strong>the optimal temperature for a given eval set does not only depend on the mean and variance of the task difficulty distribution - it depends on the form of the tail.</strong> Two eval sets with identical first and second moments but different tail shapes will require qualitatively different temperature-scaling strategies as \(k\) grows.</p>

<h3 id="power-law-task-difficulty">Power-law task difficulty</h3>

<p>Power-law distributions have even fatter tails, providing a further test of this finding. Here the scaling behavior with respect to \(k\) does not appear at low \(k\) and low \(pass@k\) rates - it only emerges at \(k \approx 20\). (Note: task_stddev_trunc is the equivalent of task_spread here, calculated from the actual truncated distribution since truncation has a much stronger effect on power-law than on Gaussian/exponential distributions.)</p>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <div>
    <img src="/assets/images_2025-06-01/image_9.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_10.png" />
  </div>
</div>

<h3 id="comparing-all-three-task-distributions">Comparing all three task distributions</h3>

<p>The plots below overlay \(T^*(k)\) for all three distribution families. Gaussian and power-law task sets can have similar \(pass@k\) for \(k&lt;10\), but diverge sharply at larger \(k\). Performance on the Gaussian set saturates by \(k \sim O(100)\), requiring only a modest increase in \(T\) to pass the hardest tasks. For the power-law set, \(T\) must increase by several orders of magnitude to reach the hardest tasks, and even then \(pass@k\) rises slowly with \(k\).</p>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <div>
    <img src="/assets/images_2025-06-01/image.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_1.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_2.png" />
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_3.png" />
  </div>
</div>

<h2 id="deciding-how-large-to-make-k">Deciding how large to make k</h2>

<p>When planning experiments it is useful to estimate the minimum \(k\) necessary to reach a desired \(pass@k\) score. Comparisons between models are most informative when \(pass@k\) values are near 0.5 rather than clustered near 0 or 1. The plots below show that in many cases there is a linear relationship \(\log(k) \sim \text{logit}(pass@k)\). This suggests that \(pass@k\) for large \(k\) can be extrapolated from a small number of samples, substantially reducing compute requirements.</p>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
  <div>
    <img src="/assets/images_2025-06-01/image_11.png" />
    <p><em>Gaussian task distribution</em></p>
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_12.png" />
    <p><em>Exponential task distribution</em></p>
  </div>
  <div>
    <img src="/assets/images_2025-06-01/image_13.png" />
    <p><em>Power law task distribution</em></p>
  </div>
  <div></div>
</div>

<h1 id="implications">Implications</h1>

<p>The main practical takeaways from this analysis:</p>

<ul>
  <li><strong>Temperature tuning for pass@k should account for the difficulty distribution of the eval set.</strong> Using a fixed temperature across benchmarks with different tail characteristics will be systematically suboptimal. Fat-tailed benchmarks (where a few tasks are much harder than the rest) require substantially higher temperatures at large \(k\).</li>
  <li><strong>The \(k\)-exponent of \(T^*(k)\) serves as a fingerprint of the eval set’s difficulty distribution.</strong> Measuring this exponent on a real benchmark could reveal whether its difficulty distribution is closer to Gaussian, exponential, or power-law - information that is otherwise hard to extract.</li>
  <li><strong>Extrapolating pass@k via the log-logit relationship</strong> could reduce the number of generations needed to estimate performance at large \(k\), making high-\(k\) evaluation more practical.</li>
  <li><strong>When comparing models on pass@k, the choice of \(k\) and \(T\) is not neutral.</strong> Two models may rank differently depending on these choices, and the degree to which rankings are sensitive to \((k, T)\) depends on the tail of the benchmark’s difficulty distribution.</li>
</ul>]]></content><author><name></name></author><category term="scaling" /></entry><entry><title type="html">Breaking Down DeepMind’s AlphaTensor</title><link href="http://localhost:4000/alphatensor/2023/07/13/alphatensor.html" rel="alternate" type="text/html" title="Breaking Down DeepMind’s AlphaTensor" /><published>2023-07-13T00:00:00-07:00</published><updated>2023-07-13T00:00:00-07:00</updated><id>http://localhost:4000/alphatensor/2023/07/13/alphatensor</id><content type="html" xml:base="http://localhost:4000/alphatensor/2023/07/13/alphatensor.html"><![CDATA[<p>(NOTE: see recent post  on <a href="% post_url 2025-06-01-pass_at_k_scaling %}">scaling laws for pass@k</a>)</p>

<ul id="markdown-toc">
  <li><a href="#doing-arithmetic-efficiently" id="markdown-toc-doing-arithmetic-efficiently">Doing Arithmetic Efficiently</a>    <ul>
      <li><a href="#matrix-multiplication" id="markdown-toc-matrix-multiplication">Matrix Multiplication</a></li>
      <li><a href="#matrix-multiplication-can-be-expressed-as-a-tensor" id="markdown-toc-matrix-multiplication-can-be-expressed-as-a-tensor">Matrix Multiplication can be Expressed as a Tensor</a></li>
      <li><a href="#matrix-multiplication-algorithms-are-tensor-decompositions" id="markdown-toc-matrix-multiplication-algorithms-are-tensor-decompositions">Matrix Multiplication Algorithms are Tensor Decompositions</a></li>
    </ul>
  </li>
  <li><a href="#implementing-alphatensor" id="markdown-toc-implementing-alphatensor">Implementing AlphaTensor</a>    <ul>
      <li><a href="#reward-function" id="markdown-toc-reward-function">Reward function</a></li>
      <li><a href="#supervised-learning" id="markdown-toc-supervised-learning">Supervised learning</a></li>
      <li><a href="#network-architecture-and-training" id="markdown-toc-network-architecture-and-training">Network Architecture and Training</a>        <ul>
          <li><a href="#training-process" id="markdown-toc-training-process">Training Process</a></li>
          <li><a href="#torso" id="markdown-toc-torso">Torso</a></li>
          <li><a href="#policy-head" id="markdown-toc-policy-head">Policy Head</a></li>
          <li><a href="#value-head" id="markdown-toc-value-head">Value Head</a></li>
        </ul>
      </li>
      <li><a href="#monte-carlo-tree-search" id="markdown-toc-monte-carlo-tree-search">Monte Carlo Tree Search</a>        <ul>
          <li><a href="#policy-improvement" id="markdown-toc-policy-improvement">Policy Improvement</a></li>
        </ul>
      </li>
      <li><a href="#additional-details" id="markdown-toc-additional-details">Additional Details</a></li>
    </ul>
  </li>
</ul>

<p>DeepMind’s AlphaTensor  (<a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">blog</a>, <a href="https://www.nature.com/articles/s41586-022-05172-4">paper</a>), introduced in October 2022, uses deep reinforcement learning to discover efficient algorithms for matrix multiplication. It has, perhaps understandably, not received the same level of attention as recent advances in generative AI. However, there are a few aspects of this work which make it a particularly interesting development in deep learning:</p>
<ul>
  <li>The complexity of the problem is rooted in its fundamental mathematical structure, not in extracting information from large empirical (i.e. social or physical) data sets such as text, image, or omics data.</li>
  <li>The action space is much larger (\(10^{10}\times\) larger!) than that of games like chess and Go, making it extremely challenging to search the game tree efficiently. The number of algorithms discovered demonstrates that this area is far richer than was previously understood.</li>
  <li>AlphaTensor combines multiple state-of-the-art techniques to tackle the problem, including a transformer network to select actions from a high dimensional, discrete space and Monte Carlo tree search (MCTS) to solve the reinforcement learning problem.</li>
</ul>

<p>In this post I break down the matrix multiplication problem and walk through <a href="https://github.com/kurtosis/mat_mul">my implementation</a> of AlphaTensor. (Most figures below are from the AlphaTensor paper.)</p>

<h1 id="doing-arithmetic-efficiently">Doing Arithmetic Efficiently</h1>

<p>It may seem surprising that the standard way of doing matrix multiplication is not optimal, but this stems from two facts:</p>
<ol>
  <li>Multiplication is a more expensive operation than addition.</li>
  <li>For large arithmetic calculations, there are many sequences of operations which produce the correct result.</li>
</ol>

<p>To illustrate (1), try calculating each of the following in your head:</p>
<center> $$15047 + 30821$$ </center>
<center> $$15047 \times 30821$$ </center>
<p>It should be clear that multiplication is more work!</p>

<p>More precisely, consider two integers, each of \(n\) digits (or bits). Using standard “pencil-and-paper” methods, the time complexity of adding them is \(O(n)\) - we first add the digits in the ones place and carry, then those in the tens place, etc. The time complexity of multiplying them is \(O(n^2)\) - we multiply the ones digit of the first number by every digit in the second number, then do the same for the tens digit, etc. Alternatively, if we denote the value of an integer as \(N\) then addition is \(O(log(N))\) and multiplication is \(O(log^2(N))\). The key point is that the complexity of standard multiplication scales as the square of the complexity of addition. (However, faster <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions">multiplication algorithms</a> exist which are \(O(n \space log(n))\), still slower than addition.)</p>

<p>To illustrate (2), consider the example (courtesy of <a href="https://www.assemblyai.com/blog/deepminds-alphatensor-explained/">Assembly AI</a>) of computing the difference of two squares, \(a^2 - b^2\). Both of these algorithms produce the correct result:</p>

<p>\(c_1 = a \cdot a\)<br />
\(c_2 = b \cdot b\)<br />
return \(c_1 - c_2\)</p>

<p>\(d_1 = a + b\)<br />
\(d_2 = a - b\)<br />
return \(d_1 \cdot d_2\)</p>

<p>Though they each involve three operations, the latter only requires one multiplication and is  faster. For matrix multiplication, there is a combinatorially large number of such “calculation paths” to consider.</p>

<h2 id="matrix-multiplication">Matrix Multiplication</h2>

<p>Consider the matrix multiplication \(C = AB\) where \(A\) and \(B\) are \(2 \times 2\) matrices. (I’ll focus on this example, but the process is generalizable to any size matrices.)</p>

<!-- ![](/assets/images/two_by_two.png){: width="300"} -->

\[\begin{pmatrix}c_1 &amp; c_2\\\ c_3 &amp; c_4\end{pmatrix} = \begin{pmatrix}a_1 &amp; a_2\\\ a_3 &amp; a_4\end{pmatrix} \cdot \begin{pmatrix}b_1 &amp; b_2\\\ b_3 &amp; b_4\end{pmatrix}\]

<p>The standard way to compute this is:</p>

\[c_1 = a_1b_1 + a_2b_3 \text{, etc.}\]

<p>Each element of the product requires two multiplications, resulting in eight multiplications overall. In 1969, mathematician <a href="https://eudml.org/doc/131927">Volker Strassen</a> showed that this can be computed using a method which requires only seven multiplications.</p>

<p><img src="/assets/images/strassen_algo.png" alt="" width="200" /></p>

<p>That’s a surprising result! But it probably also comes across as a clever one-off trick - it does not immediately suggest any structured approach to finding efficient algorithms for larger matrices. Can we do any better than blind trial-and-error or simple heuristics? We can! AlphaTensor is rooted in two observations which allow us to reframe this challenge as a problem of efficiently searching a <a href="https://en.wikipedia.org/wiki/Game_tree">game tree</a>.</p>

<h2 id="matrix-multiplication-can-be-expressed-as-a-tensor">Matrix Multiplication can be Expressed as a Tensor</h2>

<p>We can describe the multiplication \(C=AB\) by a three-dimensional tensor \(\mathcal{T}\) where the element \(t_{ijk}\) denotes the contribution of \(a_ib_j\) to \(c_k\).</p>

\[c_k = \sum_{i,j}{t_{ijk}a_ib_j}\]

<p>The elements of \(\mathcal{T}\) are all in \(\{0, 1\}\) and we can visualize it by shading each of the non-zero elements:</p>

<p><img src="/assets/images/tensor_3d.png" alt="" width="300" /></p>

<p>Note that \(c_1 = a_1b_1 + a_2b_3\) is denoted by:</p>

<p>\(t_{111} = 1\)<br />
\(t_{231} = 1\)<br />
\(t_{ij1} = 0 \space \forall \space \text{other} \space (i, j)\)<br />
and similarly for \(c_2\), \(c_3\), and \(c_4\).</p>

<h2 id="matrix-multiplication-algorithms-are-tensor-decompositions">Matrix Multiplication Algorithms are Tensor Decompositions</h2>

<p>Strassen’s algorithm can be described as performing a sequence of actions, each of which has four parts:</p>
<ol>
  <li>Compute \(u\), a linear combination of elements of \(A\). (highlighted in green above)</li>
  <li>Compute \(v\), a linear combination of elements of \(B\). (highlighted in purple above)</li>
  <li>Compute the product \(m=uv\).</li>
  <li>Add \(m\) (multiplied by a vector \({\bf w}\)) to the elements of \(C\). (highlighted in yellow above)</li>
</ol>

<p>Each action involves one scalar multiplication and Strassen’s algorithm requires seven actions. It can be expressed more compactly by stacking these into three matrices \((U,V,W)\). Each column represents one action in an algorithm to compute \(C\), defined by the column vectors \(({\bf u}\), \({\bf v}\), \({\bf w})\).</p>

<p><img src="/assets/images/uvw.png" alt="" width="300" /></p>

<p>Now for the main trick - \((U,V,W)\) can equivalently be viewed as a <a href="https://en.wikipedia.org/wiki/Tensor_decomposition">tensor decomposition</a> of \(\mathcal{T}\). Here’s what this means: consider the zero tensor  \(\mathcal{S}=0\) of same dimensions as \(\mathcal{T}\). For each set of factors \(({\bf u}\), \({\bf v}\), \({\bf w})\), perform the following update to \(\mathcal{S}\):</p>

\[s_{ijk} \leftarrow s_{ijk} + u_iv_jw_k\]

<p>After doing this for all seven columns we end up with \(\mathcal{S}=\mathcal{T}\). Thus we can reframe the problem as a single player game whose goal is to find a sequence of actions which produces a low-rank decomposition of \(\mathcal{T}\). (In practice, we set the initial state as \(\mathcal{S}=\mathcal{T}\) and subtract \(u_iv_jw_k\) at each step, so that the target state is always \(\mathcal{S}=0\).) This is referred to as TensorGame and AlphaTensor is a method for learning to play this game.</p>

<p>The table below shows the best results discovered by AlphaTensor for multiplication of various matrix sizes. Each row shows the number of actions (or rank) needed to multiply matrices of sizes \(n \times m\) and \(m \times p\). In each case, AlphaTensor was able to match or surpass the current best known algorithm - the paper even reports improvements up to size \((11, 12, 12)\). To be clear, the results themselves are not a groundbreaking improvement in computational efficiency. Rather, what is most impressive is that AlphaTensor demonstrates a promising method for searching extremely large combinatorial spaces which can be applied to many problems.</p>

<p><img src="/assets/images/best_ranks.png" alt="" width="400" /></p>

<h1 id="implementing-alphatensor">Implementing AlphaTensor</h1>

<p>The approach of AlphaTensor is broadly as follows:</p>
<ol>
  <li>Build a model to choose an action \(( {\bf u,  v,  w})\) and estimate a state-value \(Q\), given a state \(\mathcal{S}\).</li>
  <li>Define a sufficiently dense reward function to provide feedback to the model.</li>
  <li>Use a RL algorithm to explore the game tree for low-rank decompositions, guided by the model’s policy and value outputs.</li>
  <li>Supplement the RL problem with a supervised learning problem on known decompositions.</li>
</ol>

<p>In the rest of this post I’ll walk through the details of AlphaTensor. I’ll start with (2) and (4), as these are fairly straightforward. Next I’ll describe the model architecture for (1). Finally I’ll cover the Monte Carlo tree search algorithm used in (3) - this is not thoroughly described in the AlphaTensor paper and is fairly complex.</p>

<h2 id="reward-function">Reward function</h2>

<p>Since the goal is to minimize the number of actions to reach a target state, AlphaTensor provides a reward of \(-1\) for each action taken. Games are terminated when the target state is reached or after a finite number \((R_{limit})\) of steps. If \(\mathcal{S}\) is still non-zero at this point, an additional reward of \(-\gamma(\mathcal{S})\) is given, equal to “an upper bound on the rank of the terminal tensor” (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L59">code</a>). In simpler terms, \(\gamma(\mathcal{S})\) is roughly the number of non-zero entries remaining in \(\mathcal{S}\) - we know that each of these could be eliminated by a single action. Note that this terminal reward plays an important role in creating a dense reward function. Without it, the agent would only receive useful feedback when it reaches the target state within \(R_{limit}\) steps - which is effectively a sparse reward.</p>

<h2 id="supervised-learning">Supervised learning</h2>

<p>While tensor decomposition is NP-hard, it is straightforward to do the inverse: to construct a tensor \(\mathcal{D}\) from a given set of factors \(\{({\bf u}^{(r)}, {\bf v}^{(r)}, {\bf w}^{(r)})\}^R_{r=1}\). This suggests a way to create synthetic demonstrations for supervised training - a set of factors is sampled from some distribution, and the resulting tensor \(\mathcal{D}\) is given as an initial condition to the network, which is then trained to output the correct factors (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/datasets.py#L20">code</a>). AlphaTensor generates a large dataset of such demonstrations and uses a mixed training strategy, alternating between training on supervised loss on the demonstrations and reinforcement learning loss (learning to decompose \(\mathcal{T}\)). This was found to substantially outperform either strategy separately.</p>

<h2 id="network-architecture-and-training">Network Architecture and Training</h2>

<p>The AlphaTensor network consists of three components:</p>
<ol>
  <li>A torso (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L85">code</a>), which takes information about the current state \(\mathcal{S}\) and produces an embedding.</li>
  <li>A policy head (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L197">code</a>), which takes the embedding produced by the torso and generates a distribution over candidate actions.</li>
  <li>A value head (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L266">code</a>), which takes the embedding produced by the torso and generates a distribution of expected returns.</li>
</ol>

<p>In the rest of this section I’ll give a brief overview of the architecture with links to my implementation. The network is quite complex (particularly the torso) and I won’t attempt to cover all the details - to fully understand it I recommend both the paper and the pseudocode provided in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-05172-4/MediaObjects/41586_2022_5172_MOESM1_ESM.pdf">Supplementary Information</a>.</p>

<p><img src="/assets/images/network_architecture.png" alt="" width="600" /></p>

<h3 id="training-process">Training Process</h3>

<p>The network is trained on a dataset which initially consists of synthetic demonstrations. Training is done by teacher-forcing - loss is computed for each action in the ground-truth training sequence given the previous ground-truth actions. Note that loss is computed both for the policy head (based on the probability assigned to the next ground-truth action) and for the value head (comparing the output value distribution to the ground-truth rank of the current tensor state).</p>

<p>Periodically (after a given number of epochs), MCTS is performed, starting from \(\mathcal{T}\). This is the step in which we actually use the model to explore and look for a solution to the problem we are interested in. Note that MCTS uses both the policy and value heads in deciding which directions to explore. All of the played games are added to a buffer, while the game with the best reward is added to a separate buffer. Both of these buffers are merged with the training dataset and eventually training is performed on fixed proportions of synthetic demonstrations, played games, and “best” played games.</p>

<h3 id="torso">Torso</h3>

<p>The torso converts the current state \(\mathcal{S}\) (to be more precise, the past \(n\) states in the game), as well as any scalar inputs (such as the time index of the current action), to an embedding that feeds into the policy and value heads. It projects the \(4 \times 4 \times 4\) tensor onto three \(4 \times 4\) grids, one along each of its three directions. Following this, attention-based blocks (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L70">code</a>) are used to propagate information between the three grids. A block has three stages - in each stage one of the three pairs of grids is concatenated and <a href="https://arxiv.org/abs/1912.12180">axial attention</a> is applied. The output of the final block is flattened to an embedding vector which is the output of the torso.</p>

<p><img src="/assets/images/torso_architecture.png" alt="" width="800" /></p>

<h3 id="policy-head">Policy Head</h3>

<p>The policy head is responsible for converting the torso’s output into a distribution over the action space of the factors \(( {\bf u,  v,  w})\) which we can run backpropagation on (during the training step) and sample from (during the action step used in MCTS). However, this action space can be too large for us to represent the distribution explicitly. Consider multiplication of \(5 \times 5\) matrices. In this case each factor is of length \(25\). In the AlphaTensor paper, entries in the factors are restricted to the five values \((-2, -1, 0, 1, 2)\) and the cardinality of the action space is \(5^{3 \cdot 25} \approx 10^{52}\).</p>

<p>The solution is to use a transformer architecture to represent an autoregressive policy (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L160">code</a>). In other words, an action is produced sequentially, with each token in the factors drawn from a distribution that is conditioned on the previous tokens (via self-attention), as well as on the embedding produced by the torso (via cross-attention). Naively, we might treat each of the \(75\) entries in the three factors as a token. However, now we have moved from an enormous action space to the opposite extreme, a transformer with a vocabulary size of only \(5\). Recall that transformers learn embeddings for each “word” in the vocabulary- the benefit of this is most apparent for large vocabularies. Note that we can represent sequential data using different n-gram representations which trade off between vocabulary size and sequence length. In this example, we can split the factors into chunks of 5 entries (5-grams) and represent each chunk as a token. With this approach, the vocabulary size (i.e. the number of distinct values a chunk can take on) increases to \(5^5 = 3125\) and the sequence length decreases from \(75\) to \(15\). This vocabulary size is still small enough to learn embeddings over, but we have also reduced the context length that the transformer must learn to attend to.</p>

<p><img src="/assets/images/policy_head.png" alt="" width="600" /></p>

<h3 id="value-head">Value Head</h3>

<p>The value head is a multilayer perceptron whose output is an estimate of the distribution of returns from the current state. This is expressed as a series of evenly spaced quantile values. The value head is trained against ground truth values using quantile regression (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L283">code</a>, <a href="https://arxiv.org/abs/1710.10044">reference</a>).</p>

<p><img src="/assets/images/value_head.png" alt="" width="600" /></p>

<h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2>

<p>So far we’ve described our network architecture and a method of training it on synthetic demonstrations. But how do we actually play TensorGame and search for low-rank decompositions? AlphaTensor uses MCTS, as described in the <a href="https://www.science.org/doi/10.1126/science.aar6404">AlphaZero</a> and <a href="https://arxiv.org/abs/2104.06303">Sampled MuZero</a> papers. MCTS uses the output of the network’s policy and value heads, along with an upper-confidence bound decision rule to explore the game tree. The implementation of MCTS (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L8">code</a>) involves a fairly deep call stack, with several nested loops and can be difficult to grasp from reading the code directly. We’ll build some intuition by illustrating this graphically.</p>

<p>To start - the purpose of the MCTS step is to generate a set of games (or trajectories) which will be added to the training dataset (as mentioned above). Also, of course, this is the step in which we are hoping to discover a low-rank decomposition! Naively, we might consider producing a trajectory by sequentially sampling actions from the policy head and updating \(\mathcal{S}\). Perhaps we could generate several trajectories and add the best ones to the training buffer? Unsurprisingly, this simple approach is inefficient and MCTS offers a way to do better. It works by building a search tree (in which the nodes are states and the edges are actions) and using a decision rule to decide which branches to explore further, before finally choosing which action to take from the root state. Let’s break it down:</p>

<ol>
  <li>Initialize a tree with our initial state \(A\) as the root. We next wish to extend the tree which we do by sampling \(n_{samples} = 2\) actions from our network (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L182">code</a>), given input state \(A\). These actions produce the child states \(B\) and \(C\).<br />
<img src="/assets/images/a_c_graph.png" alt="" height="155" /></li>
  <li>To continue extending the tree we must choose which leaf node (\(B\) or \(C\)) to extend. We do this by starting at \(A\) and using a decision rule (which I explain below) to traverse the tree. In this example, the rule selects the branch \(A \rightarrow B\) (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L108">code</a>). As above, we sample \(n_{samples}\) actions at state \(B\), extending the tree to \(D\) and \(E\).<br />
<img src="/assets/images/a_e_graph.png" alt="" height="200" /></li>
  <li>We repeat the process in (2), and this time our decision rule selects \(A \rightarrow C\) instead (we’ll see why below) and we now extend the tree from \(C\).<br />
<img src="/assets/images/a_g_graph.png" alt="" height="200" /></li>
  <li>In the next iteration, we must apply the decision rule twice to reach a leaf node, selecting \(A \rightarrow C\) followed by \(C \rightarrow F\), before extending the tree from \(F\).<br />
<img src="/assets/images/a_i_graph.png" alt="" height="200" /></li>
  <li>Continue until we have extended the tree \(n_{sim}=4\) times. At this point, we are done exploration and will choose which action to take from \(A\). We do this using the decision rule again. In this illustration we select \(A \rightarrow C\) as the first action in our trajectory.<br />
<img src="/assets/images/a_c_final_action.png" alt="" height="200" /></li>
  <li>We now repeat the same process, starting from \(C\) to choose the next action in our trajectory. Rather than build a new tree from scratch, we start with the subtree below \(C\) and extend it until it has \(n_{sim}\) branch nodes.<br />
<img src="/assets/images/c_i_graph.png" alt="" height="150" /></li>
</ol>

<p>The decision rule used above (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L240">code</a>) selects the action \(a\) which maximizes the following quantity:</p>

\[Q(s,a) + c(s) \cdot \hat{\pi}(s,a) \frac{ \sqrt{\sum_b{N(s,b)}}}{1 + N(s,a)}\]

<p>where</p>
<ul>
  <li>\(Q(s,a)\) - an action value, based on the upper quantiles of the value head output.</li>
  <li>\(N(s,a)\) - the number of MC visits to the state-action pair \((s,a)\)</li>
  <li>\(\hat{\pi}(s,a)\) - empirical policy, the fraction of sampled actions from \(s\) that were equal to \(a\).</li>
  <li>\(c(s)\) an exploration factor to balance the two terms (essentially a hyperparameter)</li>
</ul>

<p>This is an upper-confidence tree bound - it favors actions which have a high value but have not been explored frequently and have a high empirical policy probability.</p>

<p>Each time the tree is extended we do a backward pass (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L219">code</a>) in which \(N(s,a)\) and \(Q(s,a)\) are updated for all nodes along the simulated trajectory.</p>

<p>DeepMind’s MCTS procedure uses \(n_{samples}=32\) and \(n_{sim}=800\), producing trees with up to 25,600 nodes. Given the enormous action space (up to \(10^{12}\) actions), this is a very small subset of the full game tree!</p>

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Using the above steps, we can generate a MCTS trajectory. We can represent this trajectory as a sequence of actions, as well as the policy probability and value of each action: \(\{(a_i, \hat{\pi}(a_i), Q(a_i)\}\)</p>

<p>We use \(\hat{\pi}(a_i)\) and \(Q(a_i)\) as target values to train the policy and value heads respectively. In other words, the network is trained to select action \(a_i\) not with probability 1, but with probability \(\hat{\pi}(a_i)\).</p>

<p>A simple approach is to use \(\hat{\pi}(a) = N(s,a)/N(s)\) as the policy, in other words the fraction of simulations from state \(s\) which visit action \(a\). Instead, AlphaTensor uses a temperature smoothing scheme to compute an improved policy (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L279">code</a>):</p>

\[\mathcal{I}\hat{\pi}(s,a) = \dfrac{[N(s,a)]^{1/\tau(s)}}{\sum_b{[N(s,b)]^{1/\tau(s)}}}\]

<p>where \(\tau(s)=\text{log }N(s)/\text{log }\bar{N}\) if \(N(s)&gt;\bar{N}\), else \(1\).</p>

<h2 id="additional-details">Additional Details</h2>

<p>The AlphaTensor paper includes some additional details which I did not implement. For completeness I mention them here:</p>
<ul>
  <li>Change of basis: \(\mathcal{T}\) is expressed in a large number of randomly generated bases and AlphaTensor plays games in all bases in parallel.</li>
  <li>Modular arithmetic: Agents are trained using either standard arithmetic or <a href="https://en.wikipedia.org/wiki/Modular_arithmetic">modular arithmetic</a>.</li>
  <li>Multiple target tensors: A single agent is trained to decompose tensors of different sizes. Since the network takes fixed-size inputs, smaller tensors are padded with zeros.</li>
</ul>]]></content><author><name></name></author><category term="alphatensor" /><summary type="html"><![CDATA[(NOTE: see recent post on scaling laws for pass@k)]]></summary></entry></feed>