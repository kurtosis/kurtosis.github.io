<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-02T01:10:07-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kurt Smith</title><subtitle>Writeups of my side projects.</subtitle><entry><title type="html">DeepMind’s AlphaTensor</title><link href="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html" rel="alternate" type="text/html" title="DeepMind’s AlphaTensor" /><published>2023-07-01T14:23:37-04:00</published><updated>2023-07-01T14:23:37-04:00</updated><id>http://localhost:4000/alphatensor/2023/07/01/alphatensor</id><content type="html" xml:base="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html">&lt;p&gt;DeepMind announced &lt;a href=&quot;https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor&quot;&gt;AlphaTensor&lt;/a&gt; in October 2022 (&lt;a href=&quot;https://www.nature.com/articles/s41586-022-05172-4&quot;&gt;Nature publication&lt;/a&gt;). I have implemented the algorithm &lt;a href=&quot;[https://github.com/kurtosis/mat_mul]&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;addition-and-multiplication&quot;&gt;Addition and Multiplication&lt;/h1&gt;

&lt;p&gt;The importance of the order of operations stems from the simple fact that multiplication is a more expensive operation than addition. To build some intuition, consider which of the following arithmetic calculations would take more effort to solve:&lt;/p&gt;

&lt;p&gt;(15047 + 30821)&lt;/p&gt;

&lt;p&gt;(15047 * 30821)&lt;/p&gt;

&lt;p&gt;Clearly the multiplication would be harder (you might even be able to do the addition in your head, but for the multiplication this would require a strong memory!).&lt;/p&gt;

&lt;p&gt;Consider two integers of N digits each. The cost of adding these numbers is O(N) - we first add the digits in the ones place, then those in the tens place, etc. The cost of multiplying the numbers is O(N^2) - we must multiply the “ones” digit of the first number by every digit in the second number, then do the same for the “tens” digit, etc. More generally, if we denote the magnitude of an integer as M then the number of digits scales as log(M). Thus we can say that addition is O(log(M)) and multiplication is O(log(M)^2). The key point here is the quadratic relation between multiplication and addition - this holds whether the numbers are of different magnitudes, are expressed in bits rather than base 10, or are floating point rather than integers.&lt;/p&gt;

&lt;p&gt;Another point to consider is the number of operations. Consider calculating the following:
(15047 + 30821) * (39012 + 82615)
Most of us would instinctively perform two additions followed by a single multiplication, rather than the more arduous task of four multiplications followed by three additions (or one addition, followed by two multiplications, followed by another addition). The key point here is that for complex arithmetic calculations there are multiple paths to arrive at the answer, some of which are more efficient than others. While each operation has a cost, generally reducing the number of multiplications is most important to reducing the overall cost.&lt;/p&gt;

&lt;h1 id=&quot;matrix-multiplication&quot;&gt;Matrix Multiplication&lt;/h1&gt;

&lt;p&gt;Consider matrix multiplication for two 2x2 matrices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/two_by_two.png&quot; alt=&quot;&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The standard way of computing this is:&lt;/p&gt;

&lt;p&gt;c1 = a1 * b1 + a2 * b3, etc.&lt;/p&gt;

&lt;p&gt;Each element of the product requires two multiplications, resulting in eight multiplications overall. In 1969, &lt;a href=&quot;[https://eudml.org/doc/131927]&quot;&gt;Strassen&lt;/a&gt; showed that this can be computed using a two-level method which requires only seven multiplications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/strassen_algo.png&quot; alt=&quot;&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In general, the standard way of multipling matrices of sizes (n x m) and (m x p), requires (n x p) x m scalar multiplications. It turns out that for most cases which have been examined, it is possible to perform the operation with substantially fewer scalar multiplications. For example, the AlphaTensor paper reported that the case (n=4, m=5, p=5) can be computed with 76 multiplications rather than 100.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/best_ranks.png&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;framing-the-tensor-product-as-a-set-of-discrete-moves&quot;&gt;Framing the tensor product as a set of discrete moves&lt;/h1&gt;

&lt;p&gt;The next step is to frame this search for “efficient” solutions with few multiplications (known as low-rank decompositions) as a problem that is amenable to machine learning. Looking at Strassen’s algorithm, this may seem challening at first glance, as there are an enormous number of discrete permutations to choose from and it is not obvious what sort of gradient we might perform gradient descent over.&lt;/p&gt;

&lt;p&gt;To start, consider describing matrix multiplication by a three-dimensional tensor, T, in which the first index refers to an element of $A$, the second index refers to an element of $B$ and the third index refers to an element of $C$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tensor_3d.png&quot; alt=&quot;&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the (2,2,2) multiplication we know that:
c1 = a1 * b1 + a2 * b3
and this is represented by the following entries in T:&lt;/p&gt;

&lt;p&gt;T(1, 1, 1) = 1&lt;/p&gt;

&lt;p&gt;T(2, 3, 1) = 1&lt;/p&gt;

&lt;p&gt;T(i, j, 1) = 0 for all other i, j&lt;/p&gt;

&lt;p&gt;we can fill out the remaining entries in T given the definitions of c2, c3, and c4.&lt;/p&gt;

&lt;p&gt;Let&lt;/p&gt;

&lt;p&gt;T_(i, j, k) += u_i * v_j * w_k&lt;/p&gt;

&lt;p&gt;Let’s start with T being a zero matrix and consider the following vectors:
u = (1, 0, 0, 1)
v = (1, 0, 0, 1)
w = (1, 0, 0, 1)&lt;/p&gt;

&lt;p&gt;Applying these to T results in:&lt;/p&gt;

&lt;p&gt;T_(1, 1, 1) = 1&lt;/p&gt;

&lt;p&gt;T_(1, 1, 4) = 1&lt;/p&gt;

&lt;p&gt;T_(1, 4, 1) = 1&lt;/p&gt;

&lt;p&gt;T_(1, 4, 4) = 1&lt;/p&gt;

&lt;p&gt;T_(4, 1, 1) = 1&lt;/p&gt;

&lt;p&gt;T_(4, 1, 4) = 1&lt;/p&gt;

&lt;p&gt;T_(4, 4, 1) = 1&lt;/p&gt;

&lt;p&gt;T_(4, 4, 4) = 1&lt;/p&gt;

&lt;p&gt;The basic step is as follows:&lt;/p&gt;

&lt;p&gt;u - add elements in A&lt;/p&gt;

&lt;p&gt;v - add elements in B&lt;/p&gt;

&lt;p&gt;multiply these two sums&lt;/p&gt;

&lt;p&gt;w - add the product to elements in C&lt;/p&gt;

&lt;p&gt;Following this method, Strassen’s algorithm can be represented by the following U, V, and W matrices:
&lt;img src=&quot;/assets/images/uvw.png&quot; alt=&quot;&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each of the seven columns defines a single multiplication which contributes to $C$.&lt;/p&gt;

&lt;h1 id=&quot;describing-this-as-a-game&quot;&gt;Describing this as a game&lt;/h1&gt;

&lt;p&gt;We can now begin to see how this might be casted into a sequential move game which is amenable to reinforcement learning. Starting with T as a zero tensor, we choose a “move” defined by the vectors {u, v, w} and update T accordingly. We continue to select moves until T is equal to the target tensor which defines our (n, m, p) matrix multiplication. At this point we have “won” the game and receive a “reward” that is equal to -1 * the number of moves we have taken (meaning that fewer moves is better.)&lt;/p&gt;

&lt;p&gt;It turns out to be more convenient to set the initial value of T to the target tensor, and subtract rather than add the {u,v,w} contributions, so that our goal for T to equal the zero tensor. In this way, the goal state is the same for any matrix multiplication we wish to solve and only the initial state differs.&lt;/p&gt;

&lt;p&gt;Learn a strategy to make a move for each input
The goal is to get a matrix to zero - we can provide some training examples
We also can let it explore and give feedback, score&lt;/p&gt;

&lt;p&gt;reward - not just all or nothing, approximate reward, upper bound&lt;/p&gt;</content><author><name></name></author><category term="alphatensor" /><summary type="html">DeepMind announced AlphaTensor in October 2022 (Nature publication). I have implemented the algorithm here</summary></entry></feed>