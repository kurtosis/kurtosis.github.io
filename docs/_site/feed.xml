<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-12T14:41:58-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kurt Smith</title><subtitle>Writeups of my side projects.</subtitle><entry><title type="html">Breaking Down DeepMind’s AlphaTensor</title><link href="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html" rel="alternate" type="text/html" title="Breaking Down DeepMind’s AlphaTensor" /><published>2023-07-01T14:23:37-04:00</published><updated>2023-07-01T14:23:37-04:00</updated><id>http://localhost:4000/alphatensor/2023/07/01/alphatensor</id><content type="html" xml:base="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html"><![CDATA[<p>DeepMind’s AlphaTensor system (<a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">blog</a>, <a href="https://www.nature.com/articles/s41586-022-05172-4">paper</a>), introduced in October, 2022 uses deep reinforcement learning to discover efficient algorithms for matrix multiplication. It has, perhaps understandably, not received the same level of attention as recent advances in generative AI. However, there are a few aspects of this work which I think make it a particularly interesting application of deep learning:</p>
<ul>
  <li>The complexity of the problem comes from its underlying mathematical structure. The challenge is not extracting information from large empirical data sets (e.g. text, image, omics) but solving an NP-hard problem. Also, unlike two-player games, such as chess and go, it cannot rely on self-play to provide continuous feedback and bootstrap improvements.</li>
  <li>AlphaTensor uses several techniques to tackle the problem, such as a transformer network to select actions from a high dimensional, discrete space and Monte Carlo tree search (MCTS) to solve the reinforcement learning problem.</li>
</ul>

<p>In this post I’ll break down the matrix multiplication problem and walk through <a href="https://github.com/kurtosis/mat_mul">my implementation</a> of AlphaTensor. (All figures below are from the AlphaTensor paper.)</p>

<h1 id="doing-arithmetic-efficiently">Doing Arithmetic Efficiently</h1>

<p>It may seem surprising that the standard way of doing matrix multiplication is not optimal, but this stems from two facts:</p>
<ol>
  <li>Multiplication is a more expensive operation than addition.</li>
  <li>For large arithmetic calculations, there are many sequences of operations which produce the correct result.</li>
</ol>

<p>To illustrate (1), consider which of the following would take you more effort to calculate:</p>
<center> $$15047 + 30821$$ </center>
<p>or</p>
<center>  $$15047 \times 30821$$ </center>
<p>At a glance it should be clear that multiplication is more work.
<!-- (you might even be able to do the addition in your head, but for the multiplication this would require a strong memory!). -->
More precisely, consider two integers each of \(n\) digits (or bits). Using standard “pencil-and-paper” methods, the time complexity of adding them is \(O(n)\) - we first add the digits in the ones place and carry, then those in the tens place, etc. The time complexity of multiplying them is \(O(n^2)\) - we multiply the “ones” digit of the first number by every digit in the second number, then do the same for the “tens” digit, etc. If we denote the value of an integer as \(N\) then the number of digits scales as \(log(N)\). Thus, addition is \(O(log(N)\) and multiplication is \(O((log(N)^2)\). The general point is that the complexity of standard multiplication scales as the square of the complexity of addition. (In truth, faster <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions">multiplication algorithms</a> exist which are \(O(n \space log(n))\), still slower than addition.)
<!-- This holds whether the numbers are of different magnitudes, are expressed in bits rather than base 10, or are floating point rather than integers. --></p>

<p>To illustrate (2), consider the example (courtesy of <a href="https://www.assemblyai.com/blog/deepminds-alphatensor-explained/">Assembly AI</a>) of computing the difference of two squares, \(a^2 - b^2\). Both of these algorithms return the correct result:</p>

<p>\(c_1 = a \cdot a\)<br />
\(c_2 = b \cdot b\)<br />
return \(c_1 - c2\)</p>

<p>\(d_1 = a + b\)<br />
\(d_2 = a - b\)<br />
return \(d_1 \cdot d_2\)</p>

<p>While they each involve three operations, the latter only requires one multiplication and is thus faster. For matrix multiplication, there is a combinatorially large number of such “paths” to consider.</p>

<h2 id="matrix-multiplication">Matrix Multiplication</h2>

<p>Consider the matrix multiplication \(C = AB\) where \(A\) and \(B\) are \(2 \times 2\) matrices.</p>

<!-- ![](/assets/images/two_by_two.png){: width="300"} -->

\[\begin{pmatrix}c_1 &amp; c_2\\\ c_3 &amp; c_4\end{pmatrix} = \begin{pmatrix}a_1 &amp; a_2\\\ a_3 &amp; a_4\end{pmatrix} \cdot \begin{pmatrix}b_1 &amp; b_2\\\ b_3 &amp; b_4\end{pmatrix}\]

<p>The standard way to compute this is:</p>

\[c_1 = a_1b_1 + a_2b_3 \text{, etc.}\]

<p>Each element of the product requires two multiplications, resulting in eight multiplications overall. In 1969, <a href="https://eudml.org/doc/131927">Strassen</a> showed that this can be computed using a two-level method which requires only seven multiplications.</p>

<p><img src="/assets/images/strassen_algo.png" alt="" width="200" /></p>

<p>That’s a surprising result! However it seems difficult to discern any structure to it and it might appear that finding such efficient algorithms for larger matrices is fundamentally a matter of trial-and-error. AlphaTensor’s approach is rooted in two observations which allow us to reframe this challenge as a problem of efficiently searching a <a href="https://en.wikipedia.org/wiki/Game_tree">game tree</a>.
<!-- In general, the standard way of multipling matrices of sizes $$(n \times m)$$ and $$(m \times p)$$, requires $$(n \times p) \times m$$ scalar multiplications. It turns out that for most cases which have been examined, it is possible to perform the operation with substantially fewer scalar multiplications. For example, the AlphaTensor paper reported that the case $$(n=4, m=5, p=5)$$ can be computed with 76 multiplications rather than 100. --></p>

<h3 id="matrix-multiplication-can-be-expressed-as-a-tensor">Matrix Multiplication can be Expressed as a Tensor</h3>

<p>We can describe the multiplication \(C=AB\) by a three-dimensional tensor \(\mathcal{T}\) where the \(t_{ijk}\) element denotes the contribution of \(a_ib_j\) to \(c_k\).
<!-- (Note that the dimensions of $$\mathcal{T}$$ are $$(n \times m) \times (m \times p) \times (n \times p)$$.) -->
\(c_k = \sum_{i,j}{t_{ijk}a_ib_j}\)</p>

<p>The elements of \(\mathcal{T}\) are all in \(\{0, 1\}\) and we can visualize it by shading each of the non-zero elements:<br />
<img src="/assets/images/tensor_3d.png" alt="" width="300" /></p>

<p>Note that \(c_1 = a_1b_1 + a_2b_3\) is denoted by:<br />
\(t_{111} = 1\)<br />
\(t_{231} = 1\)<br />
\(t_{ij1} = 0 \space \forall \space \text{other} \space (i, j)\)<br />
and similarly for \(c_2\), \(c_3\), and \(c_4\).</p>

<h3 id="matrix-multiplication-algorithms-are-tensor-decompositions">Matrix Multiplication Algorithms are Tensor Decompositions</h3>

<p>Note that Strassen’s algorithm can be described as repeatedly performing “steps”, each consisting of four operations:</p>
<ol>
  <li>Compute \(u\), a linear combination of elements of \(A\). (highlighted in green above)</li>
  <li>Compute \(v\), a linear combination of elements of \(B\). (highlighted in purple above)</li>
  <li>Compute the product \(m=uv\).</li>
  <li>Add \(m\) (multiplied by a vector \({\bf w}\)) to the elements of \(C\). (highlighted in yellow above)</li>
</ol>

<p>Each step involves one scalar multiplication and Strassen’s algorithm requires seven steps. This is a <a href="https://en.wikipedia.org/wiki/Tensor_decomposition">tensor decomposition</a> of \(\mathcal{T}\). It can be expressed more compactly by stacking the seven steps into three matrices \(U\), \(V\), and \(W\):<br />
<img src="/assets/images/uvw.png" alt="" width="300" /><br />
Each column represents one step in the algorithm, defined by the column vectors \({\bf u}\), \({\bf v}\), \({\bf w}\). An efficient algorithm can equivalently be thought of as a low-rank decomposition of \(\mathcal{T}\) - “rank” here refers to the number of columns in \(U\), \(V\), \(W\).</p>

<p>We are now on the verge of seeing how this can be reformulated as “game” which we can use deep reinforcement learning to tackle. Consider as an intial state the zero tensor  \(\mathcal{S}=0\) of same size as \(\mathcal{T}\). We can define an “action” as the process of choosing three vectors \({\bf u}\), \({\bf v}\), and \({\bf w}\), each of length 4, and performing the following update:<br />
\(s_{ijk} \leftarrow s_{ijk} + u_iv_jw_k\)</p>

<p>Finding a tensor decomposition is equivalent to discovering a sequence of actions which take us
from \(\mathcal{S}=0\) to the target state \(\mathcal{S}=\mathcal{T}\). In practice, it is more convenient to set the initial state as \(\mathcal{S}=\mathcal{T}\) and subtract, rather than add, at each step. Our goal is to find the smallest number steps necessary to arrive at \(\mathcal{S}=0\). This is referred to as TensorGame and the approach of AlphaTensor is broadly as follows:</p>

<ol>
  <li>Build a policy model to choose an action \(\{ {\bf u,  v,  w}\}\) given a state \(\mathcal{S}\).</li>
  <li>Define a sufficiently dense reward function to provide feedback to the policy model.</li>
  <li>Develop a RL/exploration algorithm to efficiently search for low-rank decompositions.</li>
  <li>Supplement the RL problem with a supervised learning problem on known decompositions.</li>
</ol>

<p><img src="/assets/images/best_ranks.png" alt="" width="400" /></p>

<h2 id="reward-function">Reward function</h2>

<p>Since the goal is to minimize the number of steps taken to reach \(\mathcal{S}=0\), AlphaTensor provides a reward of \(-1\) for each step taken. In practice, games are terminated after a finite number (\(R_{limit}\)) of steps. If we still have a non-zero tensor \(\mathcal{S}\) at this point, an additional reward of \(-\gamma(S)\) is given, equal to “an upper bound on the rank of the terminal tensor” (<a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/act.py#L59">code</a>). In simpler terms, \(\gamma(S)\) is the number of non-zero entries remaining in \(\mathcal{S}\) since we know that each of these could be eliminated by a single update. Note that this terminal reward plays an important role in creating a dense reward function. Without it, the agend would only recieve useful feedback when it reaches the zero tensor within \(R_{limit}\) steps - a sparse reward on its own.</p>

<h2 id="supervised-learning">Supervised learning</h2>

<p>While it is NP-hard to decompose a given tensor \(\mathcal{T}\) into factors, it is straightfoward to do the inverse: to construct a tensor \(D\) from a given set of factors \(\{({\bf u}^(r), {\bf v}^(r), {\bf w}^(r))\}^R_{r=1}\). This suggests a way to create synthetic demonstrations for supervised training - a set of factors is sampled from some distribution, and the related tensor \(D\) is given as an initial condition to the actor network, which is then trained to output the correct factors (<a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/datasets.py#L20">code</a>). AlphaTensor generates a large dataset of such demonstrations and uses a mixed training strategy, alternating between training on supervised loss on the demonstrations and reinforcement learning loss on the target tensor \(\mathcal{T}\). This was found to substantially outperform either strategy separately.</p>

<h2 id="network-architecture-and-training">Network Architecture and Training</h2>

<p>The AlphaTensor network consists of three components:</p>
<ol>
  <li>A <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L99">torso</a>, which takes information about the current state and produces an embedding.</li>
  <li>A <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L283">policy head</a>, which takes the embedding produced by the torso and generates a distribution over candidate actions.</li>
  <li>A <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L211">value head</a>, which takes the embedding produced by the torso and generates a distribution over candidate actions.</li>
</ol>

<p>In the rest of this secion I’ll give a brief overview of the architecture with links to my implementation. The network is quite complex (particularly the torso) and I won’t attempt to cover all the details - to fully understand it I recommend both the paper and the pseudocode provided in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-05172-4/MediaObjects/41586_2022_5172_MOESM1_ESM.pdf">Supplementary Information</a>.</p>

<p><img src="/assets/images/network_architecture.png" alt="" width="600" /></p>

<h3 id="training-process">Training Process</h3>

<p>The network is trained on a dataset which initially consists of synthetic demonstrations. Training is done by teacher-forcing - loss is computed for each action in the ground-truth training sequence given the previous ground-truth actions. Note that loss os computed both for the policy head (based on the probability assigned to the next ground-truth action) and for the value head (comparing the output value distribution to the ground-truth rank of the current tensor state).</p>

<p>Periodically (after a given number of epochs), a MCTS is performed, starting from \(\mathcal{T}\). This is the step in which we actually use the model to explore and look for a solution to the problem we are interested in. Note that MCTS uses both the policy and value heads in deciding which directions to explore. All of the played games are added to a buffer, and the game with the best reward is added to a separate buffer. Both of these buffers are merged with the training dataset and eventually training is performed on fixed proportions of synthetic demonstrations, played games, and “best” played games.</p>

<h3 id="torso">Torso</h3>

<p>The torso converts the current state of \(\mathcal{S}\) (to be more precise, its past \(n\) states), as well as any scalar inputs (such as the time index of the current action), to an embedding that feeds into the policy and value heads. It projects the \(4 \times 4 \times 4\) tensor onto three \(4 \times 4\) grids, one along each of its three directions. Following this, <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L71">attention-based blocks</a> are used to propagate information between the three grids. A <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L71">block</a> has three stages - in each stage one of the three pairs of grids is concatenated and <a href="https://arxiv.org/abs/1912.12180">axial attention</a> is applied. The output of the final block is flattened to an embedding vector which is the output of the torso.</p>

<p><img src="/assets/images/torso_architecture.png" alt="" width="800" /></p>

<h3 id="policy-head">Policy Head</h3>

<p>The policy head is responsible for converting the torso’s output into a distribution over the action space of the factors \(\{ {\bf u,  v,  w}\}\) which we can run backpropagation on (during the training step) and sample from (during the action step used in MCTS). However, this action space is far too large for us to represent the distribution explicitly. Consider multiplication of \(5 \times 5\) matrices. In this case each factor is of length \(25\). In the AlphaTensor paper, entries in the factors are restricted to the five values \((-2, -1, 0, 1, 2)\). Thus the cardinality of the action space is \(5^{3 \cdot 25} \approx 10^{52}\).</p>

<p>The solution is to use a transformer architecture to represent an autoregressive policy. In other words, an action is produced sequentially, with each token in the factors drawn from a distribution that is conditioned on the previous tokens (via self-attention), as well as on the embedding produced by the torso (via cross-attention). Naively, we might treat each of the \(75\) entries in the three factors as a token. However, now we have moved from an enormous action space to the opposite extreme, a transformer with a vocabulary size of only \(5\)! Recall that transformers learn embeddings for each “word” in the vocabulary- the benefit of this is most evident for larger vocabularies. Note that for any sequential data, we can use various representations that trade off between vocabulary size and sequence length. In this example, we can split the factors into chunks of 5 entries and represent each chunk as a token. With this approach, the vocabulary size (i.e. the number of distinct values a chunk can take on) increases to \(5^5 = 3125\) and the sequence length decreases to \(15\). The vocabulary size is still small enough to learn token embeddings over, but we have also reduced the context length that the transformer must learn to attend to.</p>

<p><img src="/assets/images/policy_head.png" alt="" width="600" /></p>

<h3 id="value-head">Value Head</h3>

<p>The value head is a multilayer perceptron whose output is an estimate of the distribution of returns from the current state. This is expressed as a series of evenly spaced quantile values. The value head is trained against ground truth values using  <a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/model.py#L300">quantile regression</a> (<a href="https://arxiv.org/abs/1710.10044">reference</a>).</p>

<p><img src="/assets/images/value_head.png" alt="" width="600" /></p>

<h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2>
<p>So far we’ve described our network architecture and a method of training it on synthetic demonstrations. But how do we actually play TensorGame and search for a low-rank decomposition of \(\mathcal{T}\)? AlphaTensor uses Monte Carlo Tree Search, as described in the <a href="https://www.science.org/doi/10.1126/science.aar6404">AlphaZero</a> and <a href="https://arxiv.org/abs/2104.06303">Sampled MuZero</a> papers. MCTS uses the output of the network’s policy and value heads, along with an upper-confidence bound decision rule to explore the game tree. The implementation of MCTS invovles a fairly deep call stack, with several nested loops and can be difficult to follow from the code alone. Let’s build some intuition by illustrating this graphically.</p>

<p>To start - the purpose of the MCTS step is to generate a set of games (or trajectories) which will be added to the training dataset (as mentioned above). Also, of course, this is the step in which we are hoping to discover a low-rank decomposition! Naively, we might consider producing a trajectory by sequentially sampling actions from the policy head and updating \(\mathcal{S}\). Perhaps we could generate several trajectories and add the best ones to the training buffer? Unsurprisingly, this simple approach is inefficient and MCTS is a way to do better. It works by building a search tree (in which the nodes are states and the edges are actions) and using a decision rule to decide which branches to explore further, before finally choosing which action to take. Let’s break it down:</p>

<ol>
  <li>Initialize a tree with our initial state \(A\) as the root. We next wish to extend the tree which we will do by sampling \(n_{samples} = 2\) actions from our network, given input state \(A\). These actions produce the child states \(B\) and \(C\).<br />
<img src="/assets/images/a_c_graph.png" alt="" height="155" /></li>
  <li>We want to continue extending the tree, so we must choose which leaf node (\(B\) or \(C\)) to extend. We do this by starting at \(A\) and using a decision rule (which I will define below) to traverse the tree. In this example, the rule selects the branch \(A \rightarrow B\). We next samples \(n_{samples}\) actions at state \(B\), extending the tree to \(D\) and \(E\).<br />
<img src="/assets/images/a_e_graph.png" alt="" height="200" /></li>
  <li>We repeat the process in (2), and this time our decision rule selects \(A \rightarrow C\) (more on why below!) and we now extend the tree from \(C\).<br />
<img src="/assets/images/a_g_graph.png" alt="" height="200" /></li>
  <li>In the next iteration, we apply the decision rule twice to reach a leaf node, selecting \(A \rightarrow C\) and then \(C \rightarrow F\), before extending the tree from \(F\).<br />
<img src="/assets/images/a_i_graph.png" alt="" height="200" /></li>
  <li>Continue until we have extended the tree \(n_{MC}=4\) times. At this point, we are done exploration and will choose the action to take from \(A\). We do this using the decision rule again. In this illustration we select \(A \rightarrow B\) as the first action in our trajectory.<br />
<img src="/assets/images/a_c_final_action.png" alt="" height="200" /></li>
  <li>We now repeat the same process, starting from \(C\) to choose the second action in our trajectory. Rather than building a new tree from scratch, we start with the subtree below \(C\) and extend it until it has \(n_{MC}\) branch points.<br />
<img src="/assets/images/c_i_graph.png" alt="" height="150" /></li>
</ol>

<p>The decision rule used above selects the action \(a\) which maximizes the following quantity: \(Q(s,a) + c(s) \cdot \hat{\pi}(s,a) \frac{ \sqrt{\sum_b{N(s,b)}}}{1 + N(s,a)}\)<br />
where</p>
<ul>
  <li>\(Q(s,a)\) - action value, generated by network</li>
  <li>\(N(s,a)\) - number of visits to the state-action pair \((s,a)\)</li>
  <li>\(\hat{\pi}(s,a)\) - empirical policy, the fraction of sampled actions from \(s\) that were \(a\).</li>
  <li>\(c(s)\) an exploration factor
This is the upper-confidence tree bound - it favors actions which have a high value but have not been expored frequently and have a high empirical policy probability.</li>
</ul>

<p>Each time the tree is extended we do a backward pass (<a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/act.py#L223">code</a>) in which \(N(s,a)\) and \(Q(s,a)\) are updated for all nodes along the simulated trajectory.</p>

<p>DeepMind’s MCTS procedure uses \(n_{samples}=32\) and \(n_{mc}=800\), producing trees with up to 25,600 nodes.</p>

<h3 id="policy-improvement">Policy Improvement</h3>
<p>Following the above steps, we can generate a trajectory via MCTS. We can represent this trajectory as a sequence of actions, as well as the policy probability and value of each action:<br />
\(\{(a_i, \hat{\pi}(a_i), Q(a_i)\}\)<br />
We will use \(\hat{\pi}(a_i)\) and \(Q(a_i)\) to train the policy and value heads respectively. A simple approach is to use \(\hat{\pi}(a) = N(s,a)/N(s)\) as the policy, in other words the fraction of trajectories from state \(s\) which visit action \(a\). Instead, AlphaTensor uses a temperature smoothing scheme to compute an improved policy (<a href="https://github.com/kurtosis/mat_mul/blob/7fa10f5fd351bff72712b122888ee220354f5e45/act.py#L308">code</a>):<br />
\(\mathcal{I}\hat{\pi}(s,a) = [N(s,a)]^{1/\tau(s)} / \sum_b{[N(s,b)]^{1/\tau(s)}}\)<br />
where<br />
\(\tau(s)=\text{log }N(s)/\text{log }\bar{N}\) if \(N(s)&gt;\bar{N}\), else \(1\).</p>]]></content><author><name></name></author><category term="alphatensor" /><summary type="html"><![CDATA[DeepMind’s AlphaTensor system (blog, paper), introduced in October, 2022 uses deep reinforcement learning to discover efficient algorithms for matrix multiplication. It has, perhaps understandably, not received the same level of attention as recent advances in generative AI. However, there are a few aspects of this work which I think make it a particularly interesting application of deep learning: The complexity of the problem comes from its underlying mathematical structure. The challenge is not extracting information from large empirical data sets (e.g. text, image, omics) but solving an NP-hard problem. Also, unlike two-player games, such as chess and go, it cannot rely on self-play to provide continuous feedback and bootstrap improvements. AlphaTensor uses several techniques to tackle the problem, such as a transformer network to select actions from a high dimensional, discrete space and Monte Carlo tree search (MCTS) to solve the reinforcement learning problem.]]></summary></entry></feed>