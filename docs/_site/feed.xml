<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-01-31T14:11:31-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kurt Smith</title><subtitle>Writeups of my side projects.</subtitle><entry><title type="html">Scaling Laws for Pass@k and Temperature</title><link href="http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling.html" rel="alternate" type="text/html" title="Scaling Laws for Pass@k and Temperature" /><published>2025-06-01T00:00:00-07:00</published><updated>2025-06-01T00:00:00-07:00</updated><id>http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling</id><content type="html" xml:base="http://localhost:4000/scaling/2025/06/01/pass_at_k_scaling.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#overview--motivation" id="markdown-toc-overview--motivation">Overview / Motivation</a></li>
  <li><a href="#toy-model" id="markdown-toc-toy-model">Toy Model</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a>    <ul>
      <li><a href="#how-does-the-optimal-t-for-an-eval-set-scale-with-k" id="markdown-toc-how-does-the-optimal-t-for-an-eval-set-scale-with-k"><em>How does the optimal \(T\) for an eval set scale with k?</em></a>        <ul>
          <li><a href="#normally-distributed-task-difficulty" id="markdown-toc-normally-distributed-task-difficulty">Normally Distributed Task Difficulty</a></li>
          <li><a href="#exponentially-distributed-task-difficulty" id="markdown-toc-exponentially-distributed-task-difficulty">Exponentially Distributed Task Difficulty</a></li>
          <li><a href="#power-law-distributed-task-difficulty" id="markdown-toc-power-law-distributed-task-difficulty">Power Law Distributed Task Difficulty</a></li>
          <li><a href="#comparing-all-three-task-distributions" id="markdown-toc-comparing-all-three-task-distributions">Comparing all three task distributions</a></li>
        </ul>
      </li>
      <li><a href="#deciding-how-large-to-make-k" id="markdown-toc-deciding-how-large-to-make-k">Deciding how large to make k</a></li>
    </ul>
  </li>
</ul>

<h1 id="summary">Summary</h1>

<ul>
  <li>\(pass@k\) is a complicated metric to reason about because it has a nonlinear dependence on \(k\) and temperature (\(T\)), which cannot easily be factored into independent dependences on \(k\) and \(T\).</li>
  <li>The optimal temperature \(T^*\) (that which maximizes \(\mathbb{E}[\text{pass@k}]\)) depends on \(k\). I present results from a toy model showing that \(T^*(k)\) follows a power law over several decades of \(k\).
    <ul>
      <li>Crucially, this behavior occurs because eval sets contain tasks of varying difficulties. We do not see it if we evaluate only a single task. (A “difficult task”” being defined as one with a low \(pass@1\).)</li>
      <li>As \(k \rightarrow \infty\) there is an asymptotic upper bound on \(T^*\) that depends only on the single most difficult task in the eval set. (Theoretically, if we had an infinite eval set with no single “hardest” task then the \(T^*(k)\) power law scaling would continue as  \(k \rightarrow \infty\) because there would always be harder tasks to pass.)</li>
    </ul>
  </li>
  <li>
    <p>However, <strong>the \(T^*(k)\)  power law also depends on the details of the eval set, specifically the tail of the distribution of task difficulty</strong>. The two plots below show \(T^*(k)\) curves for increasingly fat-tailed families of eval distributions (Gaussian &lt; exponential &lt; power law). For more fat-tailed distributions, one should increase \(T\) significantly more as \(k\) increases. Note that this is true even when comparing distributions with the same mean and variance - it is only due to the shape of the tail.</p>

    <table>
  <tr>
    <td><img src="/assets/images_2025-06-01/image.png" width="300" /></td>
    <td><img src="/assets/images_2025-06-01/image_1.png" width="300" /></td>
  </tr>
  <tr>
    <td><img src="/assets/images_2025-06-01/image_2.png" width="300" /></td>
    <td><img src="/assets/images_2025-06-01/image_3.png" width="300" /></td>
  </tr>
</table>
  </li>
</ul>

<h1 id="overview--motivation">Overview / Motivation</h1>

<p>\(pass@k\) is a widely used metric for AI benchmarks where i) model output is open-ended  with many possible formulations of a “correct” generation and ii) it is feasible to automatically verify solutions at scale. Code is one prominent domain where this is the case, and formal proofs are another (using Lean for verification). This class of benchmarks is distinct from benchmarks where valid answers must come from a limited set (e.g. multiple choice or math benchmarks where the answer must be an integer between 0 and 999) or short-form knowledge benchmarks, where there is unlikely to be much variation in correct responses (e.g. “What is the capital of France”?)</p>

<p>For these simpler benchmarks, \(accuracy\) (which I define here as \(pass@1\) for \(T=0\)) and \(perplexity\) (of a correct reference solution) may be sufficient metrics to estimate model capabilities. However they have shortcomings on open-ended benchmarks:</p>

<ol>
  <li>Accuracy is a binary variable for each task. For challenging benchmarks (or weak models) the average value may be very low (even zero), meaning that it is less sensitive to model improvements and has wide confidence intervals. By contrast, \(\mathbb{E}[\text{pass@k}]\) is continuous-valued  for each task, increases with \(k\), and can be estimated to arbitrarily high precision by producing \(n &gt; k\) generations.</li>
  <li>Perplexity is calculated on one (or at best, a few) reference solutions. Any coding task has a large number of correct solutions, due to trivial style variations (such as variable names), and possibly more significant algorithmic variations as well. During training, a model may learn to solve a task in a particular way which is not well reflected in the reference solution (imagine a model that always uses snake case while the provided solutions are in camel case).</li>
</ol>

<p>\(pass@k\) would seem to be the most useful metric to estimate performance (and track improvement) on open-ended, verifiable tasks. However, it brings some distinct challenges:</p>

<ol>
  <li><strong><em>Estimation</em></strong>: both accuracy (for \(T=0\)) and perplexity are deterministic and can be exactly calculated for a given model and task. Strictly speaking, \(pass@k\) is a value <strong><em>in expectation</em></strong> for a random sample of \(k\) generations from a model. This requires more care in how to best estimate it.</li>
  <li><strong><em>Sensitivity to temperature</em></strong>: using \(T=0\) for \(k&gt;1\) would seem suboptimal since all generations will be identical. It is not intuitively clear which \(T\) is optimizes \(pass@k\) or if comparing two models at the same \(T\) is truly apples-to-apples. (Is it possible that model A beats model B at T=0.5 but model B wins at T=1? NOTE: I think this is provably possible, and might even be fairly common))</li>
  <li><strong><em>Sensitivity to k</em></strong>: Likewise, \(pass@k\) can be thought of as curve over \(1 \leq\)k\(&lt; \infty\). It is not immediately obvious whether a particular set of \(k\) values provides the full picture when comparing two models.</li>
</ol>

<p>I developed a toy model to  dig deeper into the behavior of \(pass@k\) as a metric and provide guidance on how to use it to extract the most actionable signal for model development. The toy model is motivated by the following points that I want to better understand:</p>

<ol>
  <li>\(pass@k\) clearly depends on the strength of the model and the overall difficulty of the benchmark (the things we care most about), but it also depends on \(k\), \(T\), and the variance of difficulty of tasks in the benchmark set.</li>
  <li>For a given \(k\), there is an optimal \(T\) that gives the highest \(pass@k\). I would guess that \(T=0\) is frequently optimal for \(k=1\), and assume that the higher \(k\) is, the higher the optimal \(T\) will be. (This is basically an exploration vs exploitation trade-off)</li>
  <li>For a single task (at a fixed \(T\)) there is a simple relation \(pass@k = 1 - (1-pass@1)^k\)</li>
  <li>However, we usually report the average \(pass@k\) over a benchmark (a set of tasks) and there is no obvious relationship between \(pass@k\) and \(pass@1\) for the full set</li>
  <li>It is also not obvious how the \(pass@k\) curve will vary with \(T\) and what causes differences between different benchmarks. Do these things matter for choosing a useful metric to optimize?</li>
</ol>

<h1 id="toy-model">Toy Model</h1>

<p>For any task prompt, assume there is some non-zero probability (at \(T&gt;0\)) that a given LLM will generate a correct solution. The probability of generating a specific token \(x_i\) scales with \(T\)  as:</p>

\[p(x_i) \sim \exp(z_i / T)\]

<p>(of course \(T\) also influences the normalization constant). Note the similarity to a zero-mean normal distribution:</p>

\[p(x) \sim \exp(-x^2 / 2 \sigma^2)\]

<p>Consider a discrete normal distribution (i.e. defined only on the integers \(\mathbb{Z}\)). This is equivalent to a softmax function over \(\mathbb{Z}\) where \(z_i = -x_i^2 / 2\) and \(T = \sigma^2\).</p>

<p>Since the possible generations of an LLM form a countable set, we can view this distribution as a simplified representation of an LLM (where each integer represents one generation). In this vein, we can also create a simple representation of a task - we’ll say that a given task has a correct answer which is some integer \(c\). Then, for that task, we can generate/sample a random integer \(g\) from the model/distribution and we say it passes the task if \(g=c\). We can think of think of tasks with large \(\lvert c \rvert\) as “difficult”and tasks with small \(\lvert c \rvert\) as “easy”.</p>

<p>This may seem like a strange analogy, since neither the model nor the task have any semantic meaning, but the key point is that every task has a correct answer and the model can generate it with some probability that depends on \(T\). It might also seem strange that each task does not condition on a task-specific prompt, i.e. we always generate from the same normal distribution. Again, I think that is all we need to capture the basic notion of “task difficulty”- we don’t need to flesh out the specific details of each task, what matters is that each task has a \(pass@1\) rate of \(p_{gen}(c \mid T)\).</p>

<p>The final element of this toy model is that we want to create a set of evaluation tasks \(\{c_i\}\). As mentioned above, the scaling behavior of \(pass@k\) has a complex dependence on this “task difficulty distribution”. It’s not representative to just consider a scenario where \(c\) is the same for all tasks. I’ll create an evaluation set by sampling \(\{c_i\}\) from some distribution which I’ll call \(p_{task}(c)\) - not to be confused with \(p_{gen}\).</p>

<p>\(p_{task}(c)\) can be a discrete normal distribution, but we can also sample tasks from:</p>

<ul>
  <li>an exponential distribution:  \(p_{task}(c = j) \sim \exp(-\lvert j \rvert/\sqrt{T_{task}})\)</li>
  <li>a power law distribution: \(p_{task}(c = j) \sim \lvert j \rvert^{-\gamma}\)</li>
  <li>(note that  \(p_{task}\) refers to the probability of selecting a task to be in the eval set, while \(p_{gen}\) refers to the probability of our model passing a task it is given.)</li>
</ul>

<h1 id="results">Results</h1>

<p>A key issue for \(pass@k\) is the dependence on \(T\). For \(pass@1\) (that is, \(p_{gen}(c \mid T)\)) on a single task we can think of this as follows:</p>

<ol>
  <li>For \(T=0\), we often have \(p_{gen}(c)=0\) (if \(c\) is not the maximum-likelihood generation)</li>
  <li>If we increase \(T\), then \(p_{gen}(c \mid T)\) will at first increase. Loosely speaking, the distribution is spreading out and probability mass “flows towards”c from the higher-likelihood values closer to the mode)</li>
  <li>Eventually, \(p_{gen}(c \mid T)\) will reach some maximum value and decrease if we continue increasing \(T\). In this regime, we”re spreading out the distribution so much that more probability mass “flows away”from c towards the tails than “flows towards”it from the mode.</li>
  <li>To give a concrete illustration: for a zero-mean normal distribution, \(p(x)\) attains its largest value when \(\sigma = \lvert x \rvert\). The image below (from <a href="https://en.wikipedia.org/wiki/Normal_distribution">Wikipedia</a>) should help illustrate this.</li>
</ol>

<p><img src="/assets/images_2025-06-01/image_17.png" alt="" width="500" /></p>

<h2 id="how-does-the-optimal-t-for-an-eval-set-scale-with-k"><em>How does the optimal \(T\) for an eval set scale with k?</em></h2>

<p>For a single task, the optimal \(T\) is the same for all \(k\) (as the above points show), but for a set of tasks the answer is not immediately obvious. I created an eval set by sampling tasks from a discrete normal distribution. I then calculated  \(T^*(k)\), the temperature which maximizes \(pass@k\) for each \(k\) (using Brent’s method).</p>

<ul>
  <li>One important observation is that as \(k \rightarrow \infty\), \(pass@k\) is dominated by the contribution of the most difficult task (i.e. the largest \(\lvert c \rvert\)). Asymptotically, the optimal \(T\) is the value that maximizes \(pass@1\) for this task.</li>
  <li>At first I had planned to use this as a metric to compare different eval sets. However it is very noisy since it depends on the single most extreme task so I abandoned this idea.</li>
  <li>I noticed that for intermediate values of \(k\), optimal temperature follows a smooth power law over several decades so I focused on this instead.</li>
</ul>

<h3 id="normally-distributed-task-difficulty">Normally Distributed Task Difficulty</h3>

<p>On the left is the full plot for  \(T^*\)  vs \(k\). (\(task\_spread\) is the standard deviation of the task distribution). On the right I show only those points with \(0.03 &lt; pass@k &lt; 0.98\), to make the scaling regime clearer. \(T^*\) also follows a scaling law with respect to \(task\_spread\), as shown in the bottom plot.</p>

<p><img src="/assets/images_2025-06-01/image_4.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_5.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_6.png" alt="" /></p>

<p>I fit a regression to the data in this scaling regime and got the following relation (<em>s = task_spread</em>):</p>

\[T^* \approx 0.264*k^{0.728}*s^{1.283} \text{ ; } R^2=0.9996\]

<h3 id="exponentially-distributed-task-difficulty">Exponentially Distributed Task Difficulty</h3>

<p>Next I did the same analysis where the tasks are drawn from an exponential distribution instead of a normal distribution. It turns out that the scaling exponents are different:</p>

<p><img src="/assets/images_2025-06-01/image_7.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_8.png" alt="" /></p>

\[T^* \approx 0.117*k^{1.054}*s^{0.957} \text{ ; } R^2= 0.9989\]

<p>To expand on this point - the optimal temperature for a given eval set does not only depend on the mean and variance of the task difficulty distribution, it also depends on the form of the tail.</p>

<h3 id="power-law-distributed-task-difficulty">Power Law Distributed Task Difficulty</h3>

<p>Following this finding, I look at tasks drawn from a (truncated) power law distribution, which should have even fatter tails. In this case the scaling behavior wrt \(k\) does not appear at low \(k\) and low \(pass@k\) rates. It only seems to  kick in at \(k \approx 20\). (Note: task_stddev_trunc is the equivalent of task_spread here. I calculated it from the actual truncated dist I used, since truncation has a much stronger effect on power law than on gaussian/exponential dists)</p>

<p><img src="/assets/images_2025-06-01/image_9.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_10.png" alt="" /></p>

<h3 id="comparing-all-three-task-distributions">Comparing all three task distributions</h3>

<p>When we compare the \(T^*(k)\) for each family the differences are striking. Reproducing the plots from the introduction below, we can tell a clear story. For instance, there are seemingly similar gaussian and power law task sets that have a similar \(pass@k\) for \(k&lt;10\). However, as we increase \(k\), performance on the gaussian set saturates quickly (say by \(k \sim O(100))\), requiring only a modest increase in \(T\) to pass the hardest tasks. But for the power law set, it is necessary to increase \(T\) by several orders of magnitude to maximize \(pass@k\) (to “chase after”the hardest tasks) and even then, \(pass@k\) rises slowly with \(k\).</p>

<p><img src="/assets/images_2025-06-01/image.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_1.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_2.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_3.png" alt="" /></p>

<h2 id="deciding-how-large-to-make-k">Deciding how large to make k</h2>

<p>When planning experiments it would be useful to be able to estimate the minimum value of \(k\) neccessary to reach a desired \(pass@k\) score. For instance, if we are using \(pass@k\) as a metric to compare models, we will typically get more statistical precision (in other words, information) when values we are comparing are centered around 0.5, rather than clustered closer to 0 or 1.
The plots below show that in many cases there is a linear relationship \(\log(k) \sim \text{logit}(pass@k)\), thus it seems like we should be able to extrapolate from results at small \(k\).</p>

<p><img src="/assets/images_2025-06-01/image_11.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_12.png" alt="" /></p>

<p><img src="/assets/images_2025-06-01/image_13.png" alt="" /></p>

<p><em>Top left: Gaussian task distribution</em></p>

<p><em>Top right: Exponential task distribution</em></p>

<p><em>Bottom left: Power law task distribution</em></p>]]></content><author><name></name></author><category term="scaling" /></entry><entry><title type="html">Breaking Down DeepMind’s AlphaTensor</title><link href="http://localhost:4000/alphatensor/2023/07/13/alphatensor.html" rel="alternate" type="text/html" title="Breaking Down DeepMind’s AlphaTensor" /><published>2023-07-13T00:00:00-07:00</published><updated>2023-07-13T00:00:00-07:00</updated><id>http://localhost:4000/alphatensor/2023/07/13/alphatensor</id><content type="html" xml:base="http://localhost:4000/alphatensor/2023/07/13/alphatensor.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#doing-arithmetic-efficiently" id="markdown-toc-doing-arithmetic-efficiently">Doing Arithmetic Efficiently</a>    <ul>
      <li><a href="#matrix-multiplication" id="markdown-toc-matrix-multiplication">Matrix Multiplication</a></li>
      <li><a href="#matrix-multiplication-can-be-expressed-as-a-tensor" id="markdown-toc-matrix-multiplication-can-be-expressed-as-a-tensor">Matrix Multiplication can be Expressed as a Tensor</a></li>
      <li><a href="#matrix-multiplication-algorithms-are-tensor-decompositions" id="markdown-toc-matrix-multiplication-algorithms-are-tensor-decompositions">Matrix Multiplication Algorithms are Tensor Decompositions</a></li>
    </ul>
  </li>
  <li><a href="#implementing-alphatensor" id="markdown-toc-implementing-alphatensor">Implementing AlphaTensor</a>    <ul>
      <li><a href="#reward-function" id="markdown-toc-reward-function">Reward function</a></li>
      <li><a href="#supervised-learning" id="markdown-toc-supervised-learning">Supervised learning</a></li>
      <li><a href="#network-architecture-and-training" id="markdown-toc-network-architecture-and-training">Network Architecture and Training</a>        <ul>
          <li><a href="#training-process" id="markdown-toc-training-process">Training Process</a></li>
          <li><a href="#torso" id="markdown-toc-torso">Torso</a></li>
          <li><a href="#policy-head" id="markdown-toc-policy-head">Policy Head</a></li>
          <li><a href="#value-head" id="markdown-toc-value-head">Value Head</a></li>
        </ul>
      </li>
      <li><a href="#monte-carlo-tree-search" id="markdown-toc-monte-carlo-tree-search">Monte Carlo Tree Search</a>        <ul>
          <li><a href="#policy-improvement" id="markdown-toc-policy-improvement">Policy Improvement</a></li>
        </ul>
      </li>
      <li><a href="#additional-details" id="markdown-toc-additional-details">Additional Details</a></li>
    </ul>
  </li>
</ul>

<p>DeepMind’s AlphaTensor  (<a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">blog</a>, <a href="https://www.nature.com/articles/s41586-022-05172-4">paper</a>), introduced in October 2022, uses deep reinforcement learning to discover efficient algorithms for matrix multiplication. It has, perhaps understandably, not received the same level of attention as recent advances in generative AI. However, there are a few aspects of this work which make it a particularly interesting development in deep learning:</p>
<ul>
  <li>The complexity of the problem is rooted in its fundamental mathematical structure, not in extracting information from large empirical (i.e. social or physical) data sets such as text, image, or omics data.</li>
  <li>The action space is much larger (\(10^{10}\times\) larger!) than that of games like chess and Go, making it extremely challenging to search the game tree efficiently. The number of algorithms discovered demonstrates that this area is far richer than was previously understood.</li>
  <li>AlphaTensor combines multiple state-of-the-art techniques to tackle the problem, including a transformer network to select actions from a high dimensional, discrete space and Monte Carlo tree search (MCTS) to solve the reinforcement learning problem.</li>
</ul>

<p>In this post I break down the matrix multiplication problem and walk through <a href="https://github.com/kurtosis/mat_mul">my implementation</a> of AlphaTensor. (Most figures below are from the AlphaTensor paper.)</p>

<h1 id="doing-arithmetic-efficiently">Doing Arithmetic Efficiently</h1>

<p>It may seem surprising that the standard way of doing matrix multiplication is not optimal, but this stems from two facts:</p>
<ol>
  <li>Multiplication is a more expensive operation than addition.</li>
  <li>For large arithmetic calculations, there are many sequences of operations which produce the correct result.</li>
</ol>

<p>To illustrate (1), try calculating each of the following in your head:</p>
<center> $$15047 + 30821$$ </center>
<center> $$15047 \times 30821$$ </center>
<p>It should be clear that multiplication is more work!</p>

<p>More precisely, consider two integers, each of \(n\) digits (or bits). Using standard “pencil-and-paper” methods, the time complexity of adding them is \(O(n)\) - we first add the digits in the ones place and carry, then those in the tens place, etc. The time complexity of multiplying them is \(O(n^2)\) - we multiply the ones digit of the first number by every digit in the second number, then do the same for the tens digit, etc. Alternatively, if we denote the value of an integer as \(N\) then addition is \(O(log(N))\) and multiplication is \(O(log^2(N))\). The key point is that the complexity of standard multiplication scales as the square of the complexity of addition. (However, faster <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions">multiplication algorithms</a> exist which are \(O(n \space log(n))\), still slower than addition.)</p>

<p>To illustrate (2), consider the example (courtesy of <a href="https://www.assemblyai.com/blog/deepminds-alphatensor-explained/">Assembly AI</a>) of computing the difference of two squares, \(a^2 - b^2\). Both of these algorithms produce the correct result:</p>

<p>\(c_1 = a \cdot a\)<br />
\(c_2 = b \cdot b\)<br />
return \(c_1 - c_2\)</p>

<p>\(d_1 = a + b\)<br />
\(d_2 = a - b\)<br />
return \(d_1 \cdot d_2\)</p>

<p>Though they each involve three operations, the latter only requires one multiplication and is  faster. For matrix multiplication, there is a combinatorially large number of such “calculation paths” to consider.</p>

<h2 id="matrix-multiplication">Matrix Multiplication</h2>

<p>Consider the matrix multiplication \(C = AB\) where \(A\) and \(B\) are \(2 \times 2\) matrices. (I’ll focus on this example, but the process is generalizable to any size matrices.)</p>

<!-- ![](/assets/images/two_by_two.png){: width="300"} -->

\[\begin{pmatrix}c_1 &amp; c_2\\\ c_3 &amp; c_4\end{pmatrix} = \begin{pmatrix}a_1 &amp; a_2\\\ a_3 &amp; a_4\end{pmatrix} \cdot \begin{pmatrix}b_1 &amp; b_2\\\ b_3 &amp; b_4\end{pmatrix}\]

<p>The standard way to compute this is:</p>

\[c_1 = a_1b_1 + a_2b_3 \text{, etc.}\]

<p>Each element of the product requires two multiplications, resulting in eight multiplications overall. In 1969, mathematician <a href="https://eudml.org/doc/131927">Volker Strassen</a> showed that this can be computed using a method which requires only seven multiplications.</p>

<p><img src="/assets/images/strassen_algo.png" alt="" width="200" /></p>

<p>That’s a surprising result! But it probably also comes across as a clever one-off trick - it does not immediately suggest any structured approach to finding efficient algorithms for larger matrices. Can we do any better than blind trial-and-error or simple heuristics? We can! AlphaTensor is rooted in two observations which allow us to reframe this challenge as a problem of efficiently searching a <a href="https://en.wikipedia.org/wiki/Game_tree">game tree</a>.</p>

<h2 id="matrix-multiplication-can-be-expressed-as-a-tensor">Matrix Multiplication can be Expressed as a Tensor</h2>

<p>We can describe the multiplication \(C=AB\) by a three-dimensional tensor \(\mathcal{T}\) where the element \(t_{ijk}\) denotes the contribution of \(a_ib_j\) to \(c_k\).</p>

\[c_k = \sum_{i,j}{t_{ijk}a_ib_j}\]

<p>The elements of \(\mathcal{T}\) are all in \(\{0, 1\}\) and we can visualize it by shading each of the non-zero elements:</p>

<p><img src="/assets/images/tensor_3d.png" alt="" width="300" /></p>

<p>Note that \(c_1 = a_1b_1 + a_2b_3\) is denoted by:</p>

<p>\(t_{111} = 1\)<br />
\(t_{231} = 1\)<br />
\(t_{ij1} = 0 \space \forall \space \text{other} \space (i, j)\)<br />
and similarly for \(c_2\), \(c_3\), and \(c_4\).</p>

<h2 id="matrix-multiplication-algorithms-are-tensor-decompositions">Matrix Multiplication Algorithms are Tensor Decompositions</h2>

<p>Strassen’s algorithm can be described as performing a sequence of actions, each of which has four parts:</p>
<ol>
  <li>Compute \(u\), a linear combination of elements of \(A\). (highlighted in green above)</li>
  <li>Compute \(v\), a linear combination of elements of \(B\). (highlighted in purple above)</li>
  <li>Compute the product \(m=uv\).</li>
  <li>Add \(m\) (multiplied by a vector \({\bf w}\)) to the elements of \(C\). (highlighted in yellow above)</li>
</ol>

<p>Each action involves one scalar multiplication and Strassen’s algorithm requires seven actions. It can be expressed more compactly by stacking these into three matrices \((U,V,W)\). Each column represents one action in an algorithm to compute \(C\), defined by the column vectors \(({\bf u}\), \({\bf v}\), \({\bf w})\).</p>

<p><img src="/assets/images/uvw.png" alt="" width="300" /></p>

<p>Now for the main trick - \((U,V,W)\) can equivalently be viewed as a <a href="https://en.wikipedia.org/wiki/Tensor_decomposition">tensor decomposition</a> of \(\mathcal{T}\). Here’s what this means: consider the zero tensor  \(\mathcal{S}=0\) of same dimensions as \(\mathcal{T}\). For each set of factors \(({\bf u}\), \({\bf v}\), \({\bf w})\), perform the following update to \(\mathcal{S}\):</p>

\[s_{ijk} \leftarrow s_{ijk} + u_iv_jw_k\]

<p>After doing this for all seven columns we end up with \(\mathcal{S}=\mathcal{T}\). Thus we can reframe the problem as a single player game whose goal is to find a sequence of actions which produces a low-rank decomposition of \(\mathcal{T}\). (In practice, we set the initial state as \(\mathcal{S}=\mathcal{T}\) and subtract \(u_iv_jw_k\) at each step, so that the target state is always \(\mathcal{S}=0\).) This is referred to as TensorGame and AlphaTensor is a method for learning to play this game.</p>

<p>The table below shows the best results discovered by AlphaTensor for multiplication of various matrix sizes. Each row shows the number of actions (or rank) needed to multiply matrices of sizes \(n \times m\) and \(m \times p\). In each case, AlphaTensor was able to match or surpass the current best known algorithm - the paper even reports improvements up to size \((11, 12, 12)\). To be clear, the results themselves are not a groundbreaking improvement in computational efficiency. Rather, what is most impressive is that AlphaTensor demonstrates a promising method for searching extremely large combinatorial spaces which can be applied to many problems.</p>

<p><img src="/assets/images/best_ranks.png" alt="" width="400" /></p>

<h1 id="implementing-alphatensor">Implementing AlphaTensor</h1>

<p>The approach of AlphaTensor is broadly as follows:</p>
<ol>
  <li>Build a model to choose an action \(( {\bf u,  v,  w})\) and estimate a state-value \(Q\), given a state \(\mathcal{S}\).</li>
  <li>Define a sufficiently dense reward function to provide feedback to the model.</li>
  <li>Use a RL algorithm to explore the game tree for low-rank decompositions, guided by the model’s policy and value outputs.</li>
  <li>Supplement the RL problem with a supervised learning problem on known decompositions.</li>
</ol>

<p>In the rest of this post I’ll walk through the details of AlphaTensor. I’ll start with (2) and (4), as these are fairly straightforward. Next I’ll describe the model architecture for (1). Finally I’ll cover the Monte Carlo tree search algorithm used in (3) - this is not thoroughly described in the AlphaTensor paper and is fairly complex.</p>

<h2 id="reward-function">Reward function</h2>

<p>Since the goal is to minimize the number of actions to reach a target state, AlphaTensor provides a reward of \(-1\) for each action taken. Games are terminated when the target state is reached or after a finite number \((R_{limit})\) of steps. If \(\mathcal{S}\) is still non-zero at this point, an additional reward of \(-\gamma(\mathcal{S})\) is given, equal to “an upper bound on the rank of the terminal tensor” (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L59">code</a>). In simpler terms, \(\gamma(\mathcal{S})\) is roughly the number of non-zero entries remaining in \(\mathcal{S}\) - we know that each of these could be eliminated by a single action. Note that this terminal reward plays an important role in creating a dense reward function. Without it, the agent would only receive useful feedback when it reaches the target state within \(R_{limit}\) steps - which is effectively a sparse reward.</p>

<h2 id="supervised-learning">Supervised learning</h2>

<p>While tensor decomposition is NP-hard, it is straightforward to do the inverse: to construct a tensor \(\mathcal{D}\) from a given set of factors \(\{({\bf u}^{(r)}, {\bf v}^{(r)}, {\bf w}^{(r)})\}^R_{r=1}\). This suggests a way to create synthetic demonstrations for supervised training - a set of factors is sampled from some distribution, and the resulting tensor \(\mathcal{D}\) is given as an initial condition to the network, which is then trained to output the correct factors (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/datasets.py#L20">code</a>). AlphaTensor generates a large dataset of such demonstrations and uses a mixed training strategy, alternating between training on supervised loss on the demonstrations and reinforcement learning loss (learning to decompose \(\mathcal{T}\)). This was found to substantially outperform either strategy separately.</p>

<h2 id="network-architecture-and-training">Network Architecture and Training</h2>

<p>The AlphaTensor network consists of three components:</p>
<ol>
  <li>A torso (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L85">code</a>), which takes information about the current state \(\mathcal{S}\) and produces an embedding.</li>
  <li>A policy head (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L197">code</a>), which takes the embedding produced by the torso and generates a distribution over candidate actions.</li>
  <li>A value head (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L266">code</a>), which takes the embedding produced by the torso and generates a distribution of expected returns.</li>
</ol>

<p>In the rest of this section I’ll give a brief overview of the architecture with links to my implementation. The network is quite complex (particularly the torso) and I won’t attempt to cover all the details - to fully understand it I recommend both the paper and the pseudocode provided in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-022-05172-4/MediaObjects/41586_2022_5172_MOESM1_ESM.pdf">Supplementary Information</a>.</p>

<p><img src="/assets/images/network_architecture.png" alt="" width="600" /></p>

<h3 id="training-process">Training Process</h3>

<p>The network is trained on a dataset which initially consists of synthetic demonstrations. Training is done by teacher-forcing - loss is computed for each action in the ground-truth training sequence given the previous ground-truth actions. Note that loss is computed both for the policy head (based on the probability assigned to the next ground-truth action) and for the value head (comparing the output value distribution to the ground-truth rank of the current tensor state).</p>

<p>Periodically (after a given number of epochs), MCTS is performed, starting from \(\mathcal{T}\). This is the step in which we actually use the model to explore and look for a solution to the problem we are interested in. Note that MCTS uses both the policy and value heads in deciding which directions to explore. All of the played games are added to a buffer, while the game with the best reward is added to a separate buffer. Both of these buffers are merged with the training dataset and eventually training is performed on fixed proportions of synthetic demonstrations, played games, and “best” played games.</p>

<h3 id="torso">Torso</h3>

<p>The torso converts the current state \(\mathcal{S}\) (to be more precise, the past \(n\) states in the game), as well as any scalar inputs (such as the time index of the current action), to an embedding that feeds into the policy and value heads. It projects the \(4 \times 4 \times 4\) tensor onto three \(4 \times 4\) grids, one along each of its three directions. Following this, attention-based blocks (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L70">code</a>) are used to propagate information between the three grids. A block has three stages - in each stage one of the three pairs of grids is concatenated and <a href="https://arxiv.org/abs/1912.12180">axial attention</a> is applied. The output of the final block is flattened to an embedding vector which is the output of the torso.</p>

<p><img src="/assets/images/torso_architecture.png" alt="" width="800" /></p>

<h3 id="policy-head">Policy Head</h3>

<p>The policy head is responsible for converting the torso’s output into a distribution over the action space of the factors \(( {\bf u,  v,  w})\) which we can run backpropagation on (during the training step) and sample from (during the action step used in MCTS). However, this action space can be too large for us to represent the distribution explicitly. Consider multiplication of \(5 \times 5\) matrices. In this case each factor is of length \(25\). In the AlphaTensor paper, entries in the factors are restricted to the five values \((-2, -1, 0, 1, 2)\) and the cardinality of the action space is \(5^{3 \cdot 25} \approx 10^{52}\).</p>

<p>The solution is to use a transformer architecture to represent an autoregressive policy (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L160">code</a>). In other words, an action is produced sequentially, with each token in the factors drawn from a distribution that is conditioned on the previous tokens (via self-attention), as well as on the embedding produced by the torso (via cross-attention). Naively, we might treat each of the \(75\) entries in the three factors as a token. However, now we have moved from an enormous action space to the opposite extreme, a transformer with a vocabulary size of only \(5\). Recall that transformers learn embeddings for each “word” in the vocabulary- the benefit of this is most apparent for large vocabularies. Note that we can represent sequential data using different n-gram representations which trade off between vocabulary size and sequence length. In this example, we can split the factors into chunks of 5 entries (5-grams) and represent each chunk as a token. With this approach, the vocabulary size (i.e. the number of distinct values a chunk can take on) increases to \(5^5 = 3125\) and the sequence length decreases from \(75\) to \(15\). This vocabulary size is still small enough to learn embeddings over, but we have also reduced the context length that the transformer must learn to attend to.</p>

<p><img src="/assets/images/policy_head.png" alt="" width="600" /></p>

<h3 id="value-head">Value Head</h3>

<p>The value head is a multilayer perceptron whose output is an estimate of the distribution of returns from the current state. This is expressed as a series of evenly spaced quantile values. The value head is trained against ground truth values using quantile regression (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/model.py#L283">code</a>, <a href="https://arxiv.org/abs/1710.10044">reference</a>).</p>

<p><img src="/assets/images/value_head.png" alt="" width="600" /></p>

<h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2>

<p>So far we’ve described our network architecture and a method of training it on synthetic demonstrations. But how do we actually play TensorGame and search for low-rank decompositions? AlphaTensor uses MCTS, as described in the <a href="https://www.science.org/doi/10.1126/science.aar6404">AlphaZero</a> and <a href="https://arxiv.org/abs/2104.06303">Sampled MuZero</a> papers. MCTS uses the output of the network’s policy and value heads, along with an upper-confidence bound decision rule to explore the game tree. The implementation of MCTS (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L8">code</a>) involves a fairly deep call stack, with several nested loops and can be difficult to grasp from reading the code directly. We’ll build some intuition by illustrating this graphically.</p>

<p>To start - the purpose of the MCTS step is to generate a set of games (or trajectories) which will be added to the training dataset (as mentioned above). Also, of course, this is the step in which we are hoping to discover a low-rank decomposition! Naively, we might consider producing a trajectory by sequentially sampling actions from the policy head and updating \(\mathcal{S}\). Perhaps we could generate several trajectories and add the best ones to the training buffer? Unsurprisingly, this simple approach is inefficient and MCTS offers a way to do better. It works by building a search tree (in which the nodes are states and the edges are actions) and using a decision rule to decide which branches to explore further, before finally choosing which action to take from the root state. Let’s break it down:</p>

<ol>
  <li>Initialize a tree with our initial state \(A\) as the root. We next wish to extend the tree which we do by sampling \(n_{samples} = 2\) actions from our network (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L182">code</a>), given input state \(A\). These actions produce the child states \(B\) and \(C\).<br />
<img src="/assets/images/a_c_graph.png" alt="" height="155" /></li>
  <li>To continue extending the tree we must choose which leaf node (\(B\) or \(C\)) to extend. We do this by starting at \(A\) and using a decision rule (which I explain below) to traverse the tree. In this example, the rule selects the branch \(A \rightarrow B\) (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L108">code</a>). As above, we sample \(n_{samples}\) actions at state \(B\), extending the tree to \(D\) and \(E\).<br />
<img src="/assets/images/a_e_graph.png" alt="" height="200" /></li>
  <li>We repeat the process in (2), and this time our decision rule selects \(A \rightarrow C\) instead (we’ll see why below) and we now extend the tree from \(C\).<br />
<img src="/assets/images/a_g_graph.png" alt="" height="200" /></li>
  <li>In the next iteration, we must apply the decision rule twice to reach a leaf node, selecting \(A \rightarrow C\) followed by \(C \rightarrow F\), before extending the tree from \(F\).<br />
<img src="/assets/images/a_i_graph.png" alt="" height="200" /></li>
  <li>Continue until we have extended the tree \(n_{sim}=4\) times. At this point, we are done exploration and will choose which action to take from \(A\). We do this using the decision rule again. In this illustration we select \(A \rightarrow C\) as the first action in our trajectory.<br />
<img src="/assets/images/a_c_final_action.png" alt="" height="200" /></li>
  <li>We now repeat the same process, starting from \(C\) to choose the next action in our trajectory. Rather than build a new tree from scratch, we start with the subtree below \(C\) and extend it until it has \(n_{sim}\) branch nodes.<br />
<img src="/assets/images/c_i_graph.png" alt="" height="150" /></li>
</ol>

<p>The decision rule used above (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L240">code</a>) selects the action \(a\) which maximizes the following quantity:</p>

\[Q(s,a) + c(s) \cdot \hat{\pi}(s,a) \frac{ \sqrt{\sum_b{N(s,b)}}}{1 + N(s,a)}\]

<p>where</p>
<ul>
  <li>\(Q(s,a)\) - an action value, based on the upper quantiles of the value head output.</li>
  <li>\(N(s,a)\) - the number of MC visits to the state-action pair \((s,a)\)</li>
  <li>\(\hat{\pi}(s,a)\) - empirical policy, the fraction of sampled actions from \(s\) that were equal to \(a\).</li>
  <li>\(c(s)\) an exploration factor to balance the two terms (essentially a hyperparameter)</li>
</ul>

<p>This is an upper-confidence tree bound - it favors actions which have a high value but have not been explored frequently and have a high empirical policy probability.</p>

<p>Each time the tree is extended we do a backward pass (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L219">code</a>) in which \(N(s,a)\) and \(Q(s,a)\) are updated for all nodes along the simulated trajectory.</p>

<p>DeepMind’s MCTS procedure uses \(n_{samples}=32\) and \(n_{sim}=800\), producing trees with up to 25,600 nodes. Given the enormous action space (up to \(10^{12}\) actions), this is a very small subset of the full game tree!</p>

<h3 id="policy-improvement">Policy Improvement</h3>

<p>Using the above steps, we can generate a MCTS trajectory. We can represent this trajectory as a sequence of actions, as well as the policy probability and value of each action: \(\{(a_i, \hat{\pi}(a_i), Q(a_i)\}\)</p>

<p>We use \(\hat{\pi}(a_i)\) and \(Q(a_i)\) as target values to train the policy and value heads respectively. In other words, the network is trained to select action \(a_i\) not with probability 1, but with probability \(\hat{\pi}(a_i)\).</p>

<p>A simple approach is to use \(\hat{\pi}(a) = N(s,a)/N(s)\) as the policy, in other words the fraction of simulations from state \(s\) which visit action \(a\). Instead, AlphaTensor uses a temperature smoothing scheme to compute an improved policy (<a href="https://github.com/kurtosis/mat_mul/blob/2ad32d231fc2d82e9ffc66a660b60581e5458bfc/act.py#L279">code</a>):</p>

\[\mathcal{I}\hat{\pi}(s,a) = \dfrac{[N(s,a)]^{1/\tau(s)}}{\sum_b{[N(s,b)]^{1/\tau(s)}}}\]

<p>where \(\tau(s)=\text{log }N(s)/\text{log }\bar{N}\) if \(N(s)&gt;\bar{N}\), else \(1\).</p>

<h2 id="additional-details">Additional Details</h2>

<p>The AlphaTensor paper includes some additional details which I did not implement. For completeness I mention them here:</p>
<ul>
  <li>Change of basis: \(\mathcal{T}\) is expressed in a large number of randomly generated bases and AlphaTensor plays games in all bases in parallel.</li>
  <li>Modular arithmetic: Agents are trained using either standard arithmetic or <a href="https://en.wikipedia.org/wiki/Modular_arithmetic">modular arithmetic</a>.</li>
  <li>Multiple target tensors: A single agent is trained to decompose tensors of different sizes. Since the network takes fixed-size inputs, smaller tensors are padded with zeros.</li>
</ul>]]></content><author><name></name></author><category term="alphatensor" /></entry></feed>