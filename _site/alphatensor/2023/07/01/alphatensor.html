<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>DeepMind’s AlphaTensor | Kurt Smith</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="DeepMind’s AlphaTensor" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="DeepMind announced AlphaTensor in October 2022 (Nature publication). I have implemented the algorithm here" />
<meta property="og:description" content="DeepMind announced AlphaTensor in October 2022 (Nature publication). I have implemented the algorithm here" />
<link rel="canonical" href="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html" />
<meta property="og:url" content="http://localhost:4000/alphatensor/2023/07/01/alphatensor.html" />
<meta property="og:site_name" content="Kurt Smith" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-07-01T14:23:37-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="DeepMind’s AlphaTensor" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-01T14:23:37-04:00","datePublished":"2023-07-01T14:23:37-04:00","description":"DeepMind announced AlphaTensor in October 2022 (Nature publication). I have implemented the algorithm here","headline":"DeepMind’s AlphaTensor","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/alphatensor/2023/07/01/alphatensor.html"},"url":"http://localhost:4000/alphatensor/2023/07/01/alphatensor.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Kurt Smith" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Kurt Smith</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">DeepMind&#39;s AlphaTensor</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-07-01T14:23:37-04:00" itemprop="datePublished">
        Jul 1, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>DeepMind announced <a href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">AlphaTensor</a> in October 2022 (<a href="https://www.nature.com/articles/s41586-022-05172-4">Nature publication</a>). I have implemented the algorithm <a href="[https://github.com/kurtosis/mat_mul]">here</a></p>

<h1 id="addition-and-multiplication">Addition and Multiplication</h1>

<p>The importance of the order of operations stems from the simple fact that multiplication is a more expensive operation than addition. To build some intuition, consider which of the following arithmetic calculations would take more effort to solve:<br />
\(15047 + 30821\)<br />
or <br />
\(15047 \times 30821\)</p>

<p>Clearly the multiplication would be harder (you might even be able to do the addition in your head, but for the multiplication this would require a strong memory!).</p>

<p>Consider two integers of N digits each. The cost of adding these numbers is \(O(N)\) - we first add the digits in the ones place, then those in the tens place, etc. The cost of multiplying the numbers is \(O(N^2)\) - we must multiply the “ones” digit of the first number by every digit in the second number, then do the same for the “tens” digit, etc. More generally, if we denote the magnitude of an integer as \(M\) then the number of digits scales as \(log(M)\). Thus, addition is \(O(log(M))\) and multiplication is \(O(log(M)^2)\). The key point here is the quadratic relation between multiplication and addition - this holds whether the numbers are of different magnitudes, are expressed in bits rather than base 10, or are floating point rather than integers.</p>

<p>Another point to consider is the number of operations. Consider calculating the following:<br />
\((15047 + 30821) \times (39012 + 82615)\)<br />
Most of us would instinctively perform two additions followed by a single multiplication, rather than the more arduous task of four multiplications followed by three additions (or one addition, followed by two multiplications, followed by another addition). The key point here is that for complex arithmetic calculations there are multiple paths to arrive at the answer, some of which are more efficient than others. While each operation has a cost, generally reducing the number of multiplications is most important to reducing the overall cost.</p>

<h1 id="matrix-multiplication">Matrix Multiplication</h1>

<p>Consider matrix multiplication \(C = AB\) where \(A\) and \(B\) are \(2 \times 2\) matrices.</p>

<!-- ![](/assets/images/two_by_two.png){: width="300"} -->

\[\begin{pmatrix}c_1 &amp; c_2\\\ c_3 &amp; c_4\end{pmatrix} = \begin{pmatrix}a_1 &amp; a_2\\\ a_3 &amp; a_4\end{pmatrix} \cdot \begin{pmatrix}b_1 &amp; b_2\\\ b_3 &amp; b_4\end{pmatrix}\]

<p>The standard way to compute this is:</p>

\[c_1 = a_1b_1 + a_2b_3 \text{, etc.}\]

<p>Each element of the product requires two multiplications, resulting in eight multiplications overall. In 1969, <a href="[https://eudml.org/doc/131927]">Strassen</a> showed that this can be computed using a two-level method which requires only seven multiplications.</p>

<p><img src="/assets/images/strassen_algo.png" alt="" width="200" /></p>

<p>In general, the standard way of multipling matrices of sizes \((n \times m)\) and \((m \times p)\), requires \((n \times p) \times m\) scalar multiplications. It turns out that for most cases which have been examined, it is possible to perform the operation with substantially fewer scalar multiplications. For example, the AlphaTensor paper reported that the case \((n=4, m=5, p=5)\) can be computed with 76 multiplications rather than 100.</p>

<p><img src="/assets/images/best_ranks.png" alt="" width="400" /></p>

<h1 id="framing-the-tensor-product-as-a-set-of-discrete-moves">Framing the tensor product as a set of discrete moves</h1>

<p>The next step is to frame this search for efficient solutions with few multiplications (known as low-rank decompositions) as a problem that is amenable to machine learning. Looking at Strassen’s algorithm, this may seem challening at first glance, as there are an enormous number of discrete permutations to choose from and it is not obvious what sort of gradient we might perform gradient descent over.</p>

<p>To start, consider describing the matrix multiplication \(C=AB\) by a three-dimensional tensor \(T\) where the element \(T_{i,j,k}\) denotes the contribution of \(a_ib_j\) to \(c_k\). (Note that the dimensions of \(T\) are \((n \times m) \times (m \times p) \times (n \times p)\).)</p>

<p><img src="/assets/images/tensor_3d.png" alt="" width="300" /></p>

<p>For example, in the \((2,2,2)\) case, \(c_1 = a_1b_1 + a_2b_3\) is denoted by:<br />
\(T_{1, 1, 1} = 1\)<br />
\(T_{2, 3, 1} = 1\)<br />
\(T_{i, j, 1} = 0 \space \forall \space \text{other} \space (i, j)\)<br />
and similarly for \(c_2\), \(c_3\), and \(c_4\).</p>

<p>Consider a process in which we first set a tensor \(S\) equal to the zero tensor and then sequentially modify it by choosing three vectors \(\bf{u}\), \(\bf{v}\), and \(\bf{w}\), each of length 4, and performing the following update:<br />
\(S_{i, j, k} \leftarrow S_{i, j, k} + u_iv_jw_k\)</p>

<p>For example, applying the following vectors to \(S=0\):<br />
\(u = (1, 0, 0, 1)\)<br />
\(v = (1, 0, 0, 1)\)<br />
\(w = (1, 0, 0, 1)\)<br />
results in:<br />
\(S_{1, 1, 1} = 1\)<br />
\(S_{1, 1, 4} = 1\)<br />
\(S_{1, 4, 1} = 1\)<br />
\(S_{1, 4, 4} = 1\)<br />
\(S_{4, 1, 1} = 1\)<br />
\(S_{4, 1, 4} = 1\)<br />
\(S_{4, 4, 1} = 1\)<br />
\(S_{4, 4, 4} = 1\)</p>

<p>Finding a more efficient matrix multiplication algorithm is equivalent to reducing the number of such updates needed to go from \(S=0\) to \(S=T\).
Each update involves a series of arithmetic operations with one scalar multiplication:
\(u\) specifies a set of elements in \(A\) to be linearly combined.
\(v\) specifies a set of elements in \(B\) to be linearly combined.
The product of these two combinations contributes to elements of \(C\) as specified by \(w\).</p>

<p>We can stack these vectors into three matrices \(U\), \(V\), and \(W\) Following this method, Strassen’s algorithm (shown above) can be represented as:<br />
<img src="/assets/images/uvw.png" alt="" width="300" /></p>

<h1 id="describing-this-as-a-game">Describing this as a game</h1>

<p>It should now be clearer how this can be cast as a reinforcement learning problem. It will be more convenient to set the initial state as \(S=T\) and subtract, rather than add, the \({u,v,w}\) contributions at each step. Our goal is to find the smallest number steps necessary to arrive at \(S=0\).</p>

<p>At first glance it might seem that there is little coherent structure to efficient algorithms and given the discrete nature of the steps it will be hard to do better than trial-and-error.
In Strassen’s algorithm for instance \(a_1b_4\) contributes to three intermediate variables (\(m_1\), \(m_3\), and \(m_5\)) despite not contributing to any of the elements of $C$.</p>

<ol>
  <li>Build a policy model to choose an action \(\{u, v, w\}\) given a state \(S\).</li>
  <li>Define a sufficiently dense reward function to provide feedback to the policy model.</li>
  <li>Develop a RL/exploration algorithm to efficiently search for low-rank decompositions.</li>
  <li>Supplement the RL problem with a supervised learning problem on known decompositions.</li>
</ol>

<h2 id="reward-function">Reward function</h2>

<p>Since our goal is to minimize the number of steps taken to reach \(SS=0\) it is natural to use a reward of $-n$ when this state is reached on the \(n\)th step. On its own, this is a very sparse reward of course - we will only receive feedback when we succeed in finding a decomposition! Practically speaking we would like to cap the length of each trajectory and still recieve useful feedback.</p>

<p>Learn a strategy to make a move for each input
The goal is to get a matrix to zero - we can provide some training examples
We also can let it explore and give feedback, score</p>

<p>reward - not just all or nothing, approximate reward, upper bound</p>


  </div><a class="u-url" href="/alphatensor/2023/07/01/alphatensor.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Writeups of my side projects.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
